{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Validation walkthrough\n",
    "\n",
    "Here we demonstrate a sample process of running the LLM model validation. The steps are as follows:\n",
    "1. Give task specifications\n",
    "2. Download dataset + prompt + model\n",
    "3. Data ETL to align input/output formats\n",
    "4. Define relevant components (customize if necessary)\n",
    "5. Execute the orchestration process\n",
    "6. Analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Task Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "\n",
    "from llm_validation.components.clients import Client\n",
    "from llm_validation.app.configs import ClientConfig\n",
    "\n",
    "\n",
    "class OpenAiClient(Client):\n",
    "    \"\"\"\n",
    "    Note that the way we use anyscale endpoints are similiar to openai so we temporarily consider openai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ClientConfig):\n",
    "        super().__init__(config)\n",
    "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.model_name = config.model_name\n",
    "        self.model_options = config.model_options\n",
    "\n",
    "    async def predict_stream(self, messages: List):\n",
    "        client = openai.OpenAI(api_key=self.api_key)\n",
    "        stream = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            **self.model_options,\n",
    "            stream=True,\n",
    "            stream_options={\"include_usage\": True},\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            if not chunk.choices:\n",
    "                continue\n",
    "            yield dict(\n",
    "                text=chunk.choices[0].delta.content,\n",
    "                raw_response=chunk,\n",
    "            )\n",
    "\n",
    "\n",
    "class CodeGenAccuracy(AccuracyMetric):\n",
    "    def __init__(self, config: MetricConfig):\n",
    "        super().__init__(config)\n",
    "        client_config = ClientConfig(\n",
    "            name=\"openai\",\n",
    "            type=\"research\",\n",
    "            model_name=\"GPT4\",\n",
    "            base_url=\"\",\n",
    "            model_options={\"temperature\": 0, \"top_p\": 1, \"max_tokens\": 1024},\n",
    "        )\n",
    "        prompt_config = PromptConfig(\n",
    "            name=\"codegen-judge\",\n",
    "            path=\"prompts/judge.yml\",\n",
    "            version=1,\n",
    "        )\n",
    "        self.client = OpenAiClient(client_config)\n",
    "        self.prompt = Prompt(prompt_config)\n",
    "\n",
    "    def grade(self, input, output: str, label: str):\n",
    "        messages = self.prompt.transform(\n",
    "            generated_code_answer=output, expected_code_answer=label\n",
    "        )\n",
    "        try:\n",
    "            result_content = self.client.sync_predict(messages)\n",
    "            result_content = json.loads(result_content[\"text\"])\n",
    "            reason = result_content[\"reason\"]\n",
    "            code_quality = result_content[\"code_quality\"]\n",
    "            response_quality = result_content[\"response_quality\"]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            reason = \"error\"\n",
    "            code_quality = \"wrong\"\n",
    "            response_quality = \"bad\"\n",
    "        return {\n",
    "            \"reason\": reason,\n",
    "            \"code_quality\": code_quality,\n",
    "            \"response_quality\": response_quality,\n",
    "        }\n",
    "\n",
    "    def aggregate(self):\n",
    "        code_quality = self.scores[\"code_quality\"]\n",
    "        response_quality = self.scores[\"response_quality\"]\n",
    "        self.stats.update(dict(Counter(code_quality)))\n",
    "        self.stats.update(dict(Counter(response_quality)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def init_research_client(config: ClientConfig) -> Client:\n",
    "    if config.name == \"anthropic\":\n",
    "        return AnthropicClient(config)\n",
    "    elif config.name == \"bedrock\":\n",
    "        return BedrockClient(config)\n",
    "    elif config.name == \"openai\":\n",
    "        return OpenAiClient(config)\n",
    "    elif config.name == \"together\":\n",
    "        return TogetherClient(config)\n",
    "    elif config.name == \"vertex\":\n",
    "        return VertexAiClient(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Research client type not supported: {config.name}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
