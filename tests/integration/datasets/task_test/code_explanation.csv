,whole_func_string,language,func_documentation_string,token_size,label
9884,"def get_trip_points(cur, route_id, offset=0, tripid_glob=''):
    """"""Get all scheduled stops on a particular route_id.

    Given a route_id, return the trip-stop-list with
    latitude/longitudes.  This is a bit more tricky than it seems,
    because we have to go from table route->trips->stop_times.  This
    functions finds an arbitrary trip (in trip table) with this route ID
    and, and then returns all stop points for that trip.

    Parameters
    ----------
    cur : sqlite3.Cursor
        cursor to sqlite3 DB containing GTFS
    route_id : string or any
        route_id to get stop points of
    offset : int
        LIMIT offset if you don't want the first trip returned.
    tripid_glob : string
        If given, allows you to limit tripids which can be selected.
        Mainly useful in debugging.

    Returns
    -------
    stop-list
        List of stops in stop-seq format.
    """"""
    extra_where = ''
    if tripid_glob:
        extra_where = ""AND trip_id GLOB '%s'"" % tripid_glob
    cur.execute('SELECT seq, lat, lon '
                'FROM (select trip_I from route '
                '      LEFT JOIN trips USING (route_I) '
                '      WHERE route_id=? %s limit 1 offset ? ) '
                'JOIN stop_times USING (trip_I) '
                'LEFT JOIN stop USING (stop_id) '
                'ORDER BY seq' % extra_where, (route_id, offset))
    stop_points = [dict(seq=row[0], lat=row[1], lon=row[2]) for row in cur]
    return stop_points",python,"Get all scheduled stops on a particular route_id.

    Given a route_id, return the trip-stop-list with
    latitude/longitudes.  This is a bit more tricky than it seems,
    because we have to go from table route->trips->stop_times.  This
    functions finds an arbitrary trip (in trip table) with this route ID
    and, and then returns all stop points for that trip.

    Parameters
    ----------
    cur : sqlite3.Cursor
        cursor to sqlite3 DB containing GTFS
    route_id : string or any
        route_id to get stop points of
    offset : int
        LIMIT offset if you don't want the first trip returned.
    tripid_glob : string
        If given, allows you to limit tripids which can be selected.
        Mainly useful in debugging.

    Returns
    -------
    stop-list
        List of stops in stop-seq format.",360,"This Python function, `get_trip_points`, retrieves all scheduled stops for a specific route from a SQLite database containing General Transit Feed Specification (GTFS) data. The function takes four parameters: a SQLite cursor, a route ID, an optional offset, and an optional trip ID pattern for filtering trips. 

The function constructs a SQL query to fetch the sequence, latitude, and longitude of each stop on the specified route. If a trip ID pattern is provided, it is included in the query to filter the trips. The offset parameter can be used to skip a certain number of trips. 

The results of the query are then transformed into a list of dictionaries, where each dictionary represents a stop point with its sequence, latitude, and longitude. This list is returned by the function."
12710,"def status(self, order_id):
        '''Checks imagery order status. There can be more than one image per
           order and this function returns the status of all images
           within the order.

           Args:
               order_id (str): The id of the order placed.

           Returns:
               List of dictionaries, one per image. Each dictionary consists
               of the keys 'acquisition_id', 'location' and 'state'.
        '''

        self.logger.debug('Get status of order ' + order_id)
        url = '%(base_url)s/order/%(order_id)s' % {
            'base_url': self.base_url, 'order_id': order_id
        }
        r = self.gbdx_connection.get(url)
        r.raise_for_status()
        return r.json().get(""acquisitions"", {})",python,"Checks imagery order status. There can be more than one image per
           order and this function returns the status of all images
           within the order.

           Args:
               order_id (str): The id of the order placed.

           Returns:
               List of dictionaries, one per image. Each dictionary consists
               of the keys 'acquisition_id', 'location' and 'state'.",167,"This code defines a method named `status` that checks the status of an imagery order based on the provided `order_id`. It logs the action, constructs a URL using the base URL of the service and the `order_id`, and then sends a GET request to that URL. If the request encounters an HTTP error, it raises an exception. Finally, it returns the JSON response from the server, specifically the ""acquisitions"" part of the response, which is a list of dictionaries containing details about each image in the order. If ""acquisitions"" is not present in the response, it returns an empty dictionary."
15734,"def reverse_guard(lst):
    """""" Reverse guard expression. not
        (@a > 5) ->  (@a =< 5)
    Args:
        lst (list): Expression
    returns:
        list
    """"""
    rev = {'<': '>=', '>': '=<', '>=': '<', '=<': '>'}
    return [rev[l] if l in rev else l for l in lst]",python,"Reverse guard expression. not
        (@a > 5) ->  (@a =< 5)
    Args:
        lst (list): Expression
    returns:
        list",88,"This Python function, `reverse_guard`, takes a list of expressions as an argument. It returns a new list where certain elements are replaced based on a dictionary `rev`. The dictionary `rev` maps certain comparison operators to their reverse counterparts. If an element in the list matches a key in the dictionary, it is replaced with the corresponding value. If it doesn't match any key, it remains the same. This function is used to reverse the direction of comparison operators in the list."
21810,"def geom_to_xml_element(geom):
    """"""Transform a GEOS or OGR geometry object into an lxml Element
    for the GML geometry.""""""
    if geom.srs.srid != 4326:
        raise NotImplementedError(""Only WGS 84 lat/long geometries (SRID 4326) are supported."")
    # GeoJSON output is far more standard than GML, so go through that
    return geojson_to_gml(json.loads(geom.geojson))",python,"Transform a GEOS or OGR geometry object into an lxml Element
    for the GML geometry.",99,"This code defines a function `geom_to_xml_element` that transforms a GEOS or OGR geometry object into an lxml Element for the GML geometry. The function only supports WGS 84 lat/long geometries (SRID 4326) and raises a `NotImplementedError` if any other type is provided. The transformation is done by converting the geometry object to GeoJSON format first, which is more standard than GML, and then converting this GeoJSON to GML."
1863,"def get_opcodes(self):
        """"""Return list of 5-tuples describing how to turn a into b.

        Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple
        has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the
        tuple preceding it, and likewise for j1 == the previous j2.

        The tags are strings, with these meanings:

        'replace':  a[i1:i2] should be replaced by b[j1:j2]
        'delete':   a[i1:i2] should be deleted.
                    Note that j1==j2 in this case.
        'insert':   b[j1:j2] should be inserted at a[i1:i1].
                    Note that i1==i2 in this case.
        'equal':    a[i1:i2] == b[j1:j2]

        >>> a = ""qabxcd""
        >>> b = ""abycdf""
        >>> s = SequenceMatcher(None, a, b)
        >>> for tag, i1, i2, j1, j2 in s.get_opcodes():
        ...    print (""%7s a[%d:%d] (%s) b[%d:%d] (%s)"" %
        ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))
         delete a[0:1] (q) b[0:0] ()
          equal a[1:3] (ab) b[0:2] (ab)
        replace a[3:4] (x) b[2:3] (y)
          equal a[4:6] (cd) b[3:5] (cd)
         insert a[6:6] () b[5:6] (f)
        """"""

        if self.opcodes is not None:
            return self.opcodes
        i = j = 0
        self.opcodes = answer = []
        for ai, bj, size in self.get_matching_blocks():
            # invariant:  we've pumped out correct diffs to change
            # a[:i] into b[:j], and the next matching block is
            # a[ai:ai+size] == b[bj:bj+size].  So we need to pump
            # out a diff to change a[i:ai] into b[j:bj], pump out
            # the matching block, and move (i,j) beyond the match
            tag = ''
            if i < ai and j < bj:
                tag = 'replace'
            elif i < ai:
                tag = 'delete'
            elif j < bj:
                tag = 'insert'
            if tag:
                answer.append( (tag, i, ai, j, bj) )
            i, j = ai+size, bj+size
            # the list of matching blocks is terminated by a
            # sentinel with size 0
            if size:
                answer.append( ('equal', ai, i, bj, j) )
        return answer",python,"Return list of 5-tuples describing how to turn a into b.

        Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple
        has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the
        tuple preceding it, and likewise for j1 == the previous j2.

        The tags are strings, with these meanings:

        'replace':  a[i1:i2] should be replaced by b[j1:j2]
        'delete':   a[i1:i2] should be deleted.
                    Note that j1==j2 in this case.
        'insert':   b[j1:j2] should be inserted at a[i1:i1].
                    Note that i1==i2 in this case.
        'equal':    a[i1:i2] == b[j1:j2]

        >>> a = ""qabxcd""
        >>> b = ""abycdf""
        >>> s = SequenceMatcher(None, a, b)
        >>> for tag, i1, i2, j1, j2 in s.get_opcodes():
        ...    print (""%7s a[%d:%d] (%s) b[%d:%d] (%s)"" %
        ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))
         delete a[0:1] (q) b[0:0] ()
          equal a[1:3] (ab) b[0:2] (ab)
        replace a[3:4] (x) b[2:3] (y)
          equal a[4:6] (cd) b[3:5] (cd)
         insert a[6:6] () b[5:6] (f)",664,"This Python function, `get_opcodes`, is part of the `SequenceMatcher` class, which is used for comparing pairs of sequences of any type. The function returns a list of 5-tuple instructions on how to transform the first sequence into the second one. 

The tuples are in the form of (tag, i1, i2, j1, j2), where 'tag' is a string that can be 'replace', 'delete', 'insert', or 'equal', representing the operation needed to be done on the first sequence to make it identical to the second one. The other four elements in the tuple represent the indices in the sequences where the operation should be applied.

The function works by iterating over matching blocks between the two sequences, and for each block, it determines the operation needed (replace, delete, insert, or equal) to make the corresponding parts of the sequences identical. These operations are then appended to the `opcodes` list, which is returned at the end."
33,"def learn(network, env, seed=None, nsteps=20, total_timesteps=int(80e6), q_coef=0.5, ent_coef=0.01,
          max_grad_norm=10, lr=7e-4, lrschedule='linear', rprop_epsilon=1e-5, rprop_alpha=0.99, gamma=0.99,
          log_interval=100, buffer_size=50000, replay_ratio=4, replay_start=10000, c=10.0,
          trust_region=True, alpha=0.99, delta=1, load_path=None, **network_kwargs):

    '''
    Main entrypoint for ACER (Actor-Critic with Experience Replay) algorithm (https://arxiv.org/pdf/1611.01224.pdf)
    Train an agent with given network architecture on a given environment using ACER.

    Parameters:
    ----------

    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)
                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns
                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward
                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.
                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies

    env:                environment. Needs to be vectorized for parallel environment simulation.
                        The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.

    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where
                        nenv is number of environment copies simulated in parallel) (default: 20)

    nstack:             int, size of the frame stack, i.e. number of the frames passed to the step model. Frames are stacked along channel dimension
                        (last image dimension) (default: 4)

    total_timesteps:    int, number of timesteps (i.e. number of actions taken in the environment) (default: 80M)

    q_coef:             float, value function loss coefficient in the optimization objective (analog of vf_coef for other actor-critic methods)

    ent_coef:           float, policy entropy coefficient in the optimization objective (default: 0.01)

    max_grad_norm:      float, gradient norm clipping coefficient. If set to None, no clipping. (default: 10),

    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)

    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and
                        returns fraction of the learning rate (specified as lr) as output

    rprop_epsilon:      float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)

    rprop_alpha:        float, RMSProp decay parameter (default: 0.99)

    gamma:              float, reward discounting factor (default: 0.99)

    log_interval:       int, number of updates between logging events (default: 100)

    buffer_size:        int, size of the replay buffer (default: 50k)

    replay_ratio:       int, now many (on average) batches of data to sample from the replay buffer take after batch from the environment (default: 4)

    replay_start:       int, the sampling from the replay buffer does not start until replay buffer has at least that many samples (default: 10k)

    c:                  float, importance weight clipping factor (default: 10)

    trust_region        bool, whether or not algorithms estimates the gradient KL divergence between the old and updated policy and uses it to determine step size  (default: True)

    delta:              float, max KL divergence between the old policy and updated policy (default: 1)

    alpha:              float, momentum factor in the Polyak (exponential moving average) averaging of the model parameters (default: 0.99)

    load_path:          str, path to load the model from (default: None)

    **network_kwargs:               keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network
                                    For instance, 'mlp' network architecture has arguments num_hidden and num_layers.

    '''

    print(""Running Acer Simple"")
    print(locals())
    set_global_seeds(seed)
    if not isinstance(env, VecFrameStack):
        env = VecFrameStack(env, 1)

    policy = build_policy(env, network, estimate_q=True, **network_kwargs)
    nenvs = env.num_envs
    ob_space = env.observation_space
    ac_space = env.action_space

    nstack = env.nstack
    model = Model(policy=policy, ob_space=ob_space, ac_space=ac_space, nenvs=nenvs, nsteps=nsteps,
                  ent_coef=ent_coef, q_coef=q_coef, gamma=gamma,
                  max_grad_norm=max_grad_norm, lr=lr, rprop_alpha=rprop_alpha, rprop_epsilon=rprop_epsilon,
                  total_timesteps=total_timesteps, lrschedule=lrschedule, c=c,
                  trust_region=trust_region, alpha=alpha, delta=delta)

    runner = Runner(env=env, model=model, nsteps=nsteps)
    if replay_ratio > 0:
        buffer = Buffer(env=env, nsteps=nsteps, size=buffer_size)
    else:
        buffer = None
    nbatch = nenvs*nsteps
    acer = Acer(runner, model, buffer, log_interval)
    acer.tstart = time.time()

    for acer.steps in range(0, total_timesteps, nbatch): #nbatch samples, 1 on_policy call and multiple off-policy calls
        acer.call(on_policy=True)
        if replay_ratio > 0 and buffer.has_atleast(replay_start):
            n = np.random.poisson(replay_ratio)
            for _ in range(n):
                acer.call(on_policy=False)  # no simulation steps in this

    return model",python,"Main entrypoint for ACER (Actor-Critic with Experience Replay) algorithm (https://arxiv.org/pdf/1611.01224.pdf)
    Train an agent with given network architecture on a given environment using ACER.

    Parameters:
    ----------

    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)
                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns
                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward
                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.
                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies

    env:                environment. Needs to be vectorized for parallel environment simulation.
                        The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.

    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where
                        nenv is number of environment copies simulated in parallel) (default: 20)

    nstack:             int, size of the frame stack, i.e. number of the frames passed to the step model. Frames are stacked along channel dimension
                        (last image dimension) (default: 4)

    total_timesteps:    int, number of timesteps (i.e. number of actions taken in the environment) (default: 80M)

    q_coef:             float, value function loss coefficient in the optimization objective (analog of vf_coef for other actor-critic methods)

    ent_coef:           float, policy entropy coefficient in the optimization objective (default: 0.01)

    max_grad_norm:      float, gradient norm clipping coefficient. If set to None, no clipping. (default: 10),

    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)

    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and
                        returns fraction of the learning rate (specified as lr) as output

    rprop_epsilon:      float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)

    rprop_alpha:        float, RMSProp decay parameter (default: 0.99)

    gamma:              float, reward discounting factor (default: 0.99)

    log_interval:       int, number of updates between logging events (default: 100)

    buffer_size:        int, size of the replay buffer (default: 50k)

    replay_ratio:       int, now many (on average) batches of data to sample from the replay buffer take after batch from the environment (default: 4)

    replay_start:       int, the sampling from the replay buffer does not start until replay buffer has at least that many samples (default: 10k)

    c:                  float, importance weight clipping factor (default: 10)

    trust_region        bool, whether or not algorithms estimates the gradient KL divergence between the old and updated policy and uses it to determine step size  (default: True)

    delta:              float, max KL divergence between the old policy and updated policy (default: 1)

    alpha:              float, momentum factor in the Polyak (exponential moving average) averaging of the model parameters (default: 0.99)

    load_path:          str, path to load the model from (default: None)

    **network_kwargs:               keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network
                                    For instance, 'mlp' network architecture has arguments num_hidden and num_layers.",1374,"This code defines a function `learn` which is the main entry point for the ACER (Actor-Critic with Experience Replay) algorithm. The ACER algorithm is used to train an agent with a given network architecture on a given environment. 

The function takes a number of parameters including the network architecture, the environment, the number of steps of the vectorized environment per update, the total number of timesteps, the learning rate, the size of the replay buffer, and many others. 

The function first sets the global seed and builds the policy based on the provided network architecture and environment. It then creates a model and a runner for the environment and the model. If the replay ratio is greater than 0, it also creates a buffer. 

The function then enters a loop where it calls the ACER algorithm for a number of steps. If the replay ratio is greater than 0 and the buffer has at least `replay_start` samples, it also calls the ACER algorithm for a number of off-policy steps. 

Finally, the function returns the model."
17054,"def _process_files(self, record_id, data):
        """"""Snapshot bucket and add files in record during first publishing.""""""
        if self.files:
            assert not self.files.bucket.locked
            self.files.bucket.locked = True
            snapshot = self.files.bucket.snapshot(lock=True)
            data['_files'] = self.files.dumps(bucket=snapshot.id)
            yield data
            db.session.add(RecordsBuckets(
                record_id=record_id, bucket_id=snapshot.id
            ))
        else:
            yield data",python,Snapshot bucket and add files in record during first publishing.,105,"This code defines a method `_process_files` that processes files associated with a record during its first publishing. If the record has files, it locks the bucket containing the files, creates a snapshot of the bucket, and adds the serialized file data to the `data` dictionary. It then yields the `data` dictionary and adds a new `RecordsBuckets` object to the database session, linking the record ID to the snapshot bucket ID. If the record has no files, it simply yields the `data` dictionary."
4574,"def shutdown(self, hub=True, targets='all', block=False):
        """"""Shutdown the executor, including all workers and controllers.

        This is not implemented.

        Kwargs:
            - hub (Bool): Whether the hub should be shutdown, Default:True,
            - targets (list of ints| 'all'): List of block id's to kill, Default:'all'
            - block (Bool): To block for confirmations or not

        Raises:
             NotImplementedError
        """"""

        logger.info(""Attempting HighThroughputExecutor shutdown"")
        # self.outgoing_q.close()
        # self.incoming_q.close()
        self.queue_proc.terminate()
        logger.info(""Finished HighThroughputExecutor shutdown attempt"")
        return True",python,"Shutdown the executor, including all workers and controllers.

        This is not implemented.

        Kwargs:
            - hub (Bool): Whether the hub should be shutdown, Default:True,
            - targets (list of ints| 'all'): List of block id's to kill, Default:'all'
            - block (Bool): To block for confirmations or not

        Raises:
             NotImplementedError",146,"This code defines a method called `shutdown` for a class (not shown in the snippet). The method is intended to shut down the executor, which includes all workers and controllers. However, the actual implementation for this functionality is not provided in the code, hence it raises a `NotImplementedError`. 

The method accepts three optional arguments: `hub` (a boolean indicating whether the hub should be shut down), `targets` (a list of block IDs to terminate or 'all' to terminate all), and `block` (a boolean indicating whether to block for confirmations or not). 

The method logs an info message before and after attempting to terminate a process named `queue_proc`. The commented lines suggest that it might have been intended to close two queues (`outgoing_q` and `incoming_q`) as part of the shutdown process, but this is not currently implemented. The method always returns `True` after logging the shutdown attempt."
2830,"def init_rotating_logger(level, logfile, max_files, max_bytes):
  """"""Initializes a rotating logger

  It also makes sure that any StreamHandler is removed, so as to avoid stdout/stderr
  constipation issues
  """"""
  logging.basicConfig()

  root_logger = logging.getLogger()
  log_format = ""[%(asctime)s] [%(levelname)s] %(filename)s: %(message)s""

  root_logger.setLevel(level)
  handler = RotatingFileHandler(logfile, maxBytes=max_bytes, backupCount=max_files)
  handler.setFormatter(logging.Formatter(fmt=log_format, datefmt=date_format))
  root_logger.addHandler(handler)

  for handler in root_logger.handlers:
    root_logger.debug(""Associated handlers - "" + str(handler))
    if isinstance(handler, logging.StreamHandler):
      root_logger.debug(""Removing StreamHandler: "" + str(handler))
      root_logger.handlers.remove(handler)",python,"Initializes a rotating logger

  It also makes sure that any StreamHandler is removed, so as to avoid stdout/stderr
  constipation issues",181,"This code defines a function `init_rotating_logger` that initializes a rotating logger. The logger logs messages with a specific format that includes the time, log level, filename, and message. The logs are written to a file specified by `logfile`, and the file rotates when it reaches a certain size (`max_bytes`). The number of rotated files kept is determined by `max_files`. 

The function also checks for any existing `StreamHandler` instances in the logger's handlers and removes them. This is done to avoid potential issues with stdout/stderr blocking. The log level for the root logger is set to the `level` parameter passed to the function."
8246,"def drawLine(self, x0, y0, x1, y1, color=None, colorFunc=None, aa=False):
        """"""
        Draw a between x0, y0 and x1, y1 in an RGB color.

        :param colorFunc: a function that takes an integer from x0 to x1 and
            returns a color corresponding to that point
        :param aa: if True, use Bresenham's algorithm for line drawing;
            otherwise use Xiaolin Wu's algorithm
        """"""
        md.draw_line(self.set, x0, y0, x1, y1, color, colorFunc, aa)",python,"Draw a between x0, y0 and x1, y1 in an RGB color.

        :param colorFunc: a function that takes an integer from x0 to x1 and
            returns a color corresponding to that point
        :param aa: if True, use Bresenham's algorithm for line drawing;
            otherwise use Xiaolin Wu's algorithm",134,"This code defines a function named `drawLine` that draws a line between two points (x0, y0) and (x1, y1) in a given color. The color of the line can be specified directly using the `color` parameter or dynamically using the `colorFunc` parameter, which is a function that returns a color for each point from x0 to x1. The `aa` parameter determines the algorithm used for line drawing: if `aa` is True, Bresenham's algorithm is used; otherwise, Xiaolin Wu's algorithm is used. The actual line drawing is performed by the `draw_line` method of the `md` module, to which all parameters are passed."
11893,"def _easy_install(argv, python_cmd, use_sudo):
    """"""
    Install packages using easy_install

    We don't know if the easy_install command in the path will be the
    right one, so we use the setuptools entry point to call the script's
    main function ourselves.
    """"""
    command = """"""python -c ""\
        from pkg_resources import load_entry_point;\
        ez = load_entry_point('setuptools', 'console_scripts', 'easy_install');\
        ez(argv=%(argv)r)\
    """""" % locals()
    if use_sudo:
        run_as_root(command)
    else:
        run(command)",python,"Install packages using easy_install

    We don't know if the easy_install command in the path will be the
    right one, so we use the setuptools entry point to call the script's
    main function ourselves.",131,"This Python function, `_easy_install`, is used to install packages using the `easy_install` command from setuptools. It takes three arguments: `argv` (the arguments to pass to `easy_install`), `python_cmd` (unused in this snippet), and `use_sudo` (a boolean indicating whether to run the command as root). The function constructs a Python command that loads the `easy_install` entry point from setuptools and calls it with the provided arguments. If `use_sudo` is True, it runs the command as root, otherwise it runs the command normally."
17093,"def rewrite(fname, visitor, **kw):
    """"""Utility function to rewrite rows in tsv files.

    :param fname: Path of the dsv file to operate on.
    :param visitor: A callable that takes a line-number and a row as input and returns a \
    (modified) row or None to filter out the row.
    :param kw: Keyword parameters are passed through to csv.reader/csv.writer.
    """"""
    if not isinstance(fname, pathlib.Path):
        assert isinstance(fname, string_types)
        fname = pathlib.Path(fname)

    assert fname.is_file()
    with tempfile.NamedTemporaryFile(delete=False) as fp:
        tmp = pathlib.Path(fp.name)

    with UnicodeReader(fname, **kw) as reader_:
        with UnicodeWriter(tmp, **kw) as writer:
            for i, row in enumerate(reader_):
                row = visitor(i, row)
                if row is not None:
                    writer.writerow(row)
    shutil.move(str(tmp), str(fname))",python,"Utility function to rewrite rows in tsv files.

    :param fname: Path of the dsv file to operate on.
    :param visitor: A callable that takes a line-number and a row as input and returns a \
    (modified) row or None to filter out the row.
    :param kw: Keyword parameters are passed through to csv.reader/csv.writer.",202,"This Python function, `rewrite`, is designed to modify rows in a tab-separated values (TSV) file. It takes three parameters: `fname` which is the path of the file to operate on, `visitor` which is a function that takes a line number and a row as input and returns a modified row or None to filter out the row, and `kw` which are keyword parameters passed to csv.reader/csv.writer. 

The function first checks if `fname` is a Path object, if not, it converts it into one. It then asserts that `fname` is a file. It creates a temporary file and opens the original file and the temporary file. It reads from the original file and writes to the temporary file. For each row in the original file, it applies the `visitor` function. If the `visitor` function returns None, the row is filtered out, otherwise, the modified row is written to the temporary file. Finally, it replaces the original file with the temporary file."
10846,"def fit_lens_data_with_sensitivity_tracers(lens_data, tracer_normal, tracer_sensitive):
    """"""Fit lens data with a normal tracer and sensitivity tracer, to determine our sensitivity to a selection of \ 
    galaxy components. This factory automatically determines the type of fit based on the properties of the galaxies \
    in the tracers.

    Parameters
    -----------
    lens_data : lens_data.LensData or lens_data.LensDataHyper
        The lens-images that is fitted.
    tracer_normal : ray_tracing.AbstractTracer
        A tracer whose galaxies have the same model components (e.g. light profiles, mass profiles) as the \
        lens data that we are fitting.
    tracer_sensitive : ray_tracing.AbstractTracerNonStack
        A tracer whose galaxies have the same model components (e.g. light profiles, mass profiles) as the \
        lens data that we are fitting, but also addition components (e.g. mass clumps) which we measure \
        how sensitive we are too.
    """"""

    if (tracer_normal.has_light_profile and tracer_sensitive.has_light_profile) and \
            (not tracer_normal.has_pixelization and not tracer_sensitive.has_pixelization):
        return SensitivityProfileFit(lens_data=lens_data, tracer_normal=tracer_normal,
                                     tracer_sensitive=tracer_sensitive)

    elif (not tracer_normal.has_light_profile and not tracer_sensitive.has_light_profile) and \
            (tracer_normal.has_pixelization and tracer_sensitive.has_pixelization):
        return SensitivityInversionFit(lens_data=lens_data, tracer_normal=tracer_normal,
                                     tracer_sensitive=tracer_sensitive)
    else:

        raise exc.FittingException('The sensitivity_fit routine did not call a SensitivityFit class - check the '
                                   'properties of the tracers')",python,"Fit lens data with a normal tracer and sensitivity tracer, to determine our sensitivity to a selection of \ 
    galaxy components. This factory automatically determines the type of fit based on the properties of the galaxies \
    in the tracers.

    Parameters
    -----------
    lens_data : lens_data.LensData or lens_data.LensDataHyper
        The lens-images that is fitted.
    tracer_normal : ray_tracing.AbstractTracer
        A tracer whose galaxies have the same model components (e.g. light profiles, mass profiles) as the \
        lens data that we are fitting.
    tracer_sensitive : ray_tracing.AbstractTracerNonStack
        A tracer whose galaxies have the same model components (e.g. light profiles, mass profiles) as the \
        lens data that we are fitting, but also addition components (e.g. mass clumps) which we measure \
        how sensitive we are too.",363,"This Python function, `fit_lens_data_with_sensitivity_tracers`, is designed to fit lens data with a normal tracer and a sensitivity tracer. The purpose is to determine the sensitivity to a selection of galaxy components. The function takes three arguments: `lens_data`, `tracer_normal`, and `tracer_sensitive`.

The function first checks if both tracers have a light profile and do not have pixelization. If this is the case, it returns a `SensitivityProfileFit` object with the provided lens data and tracers.

If both tracers do not have a light profile but do have pixelization, it returns a `SensitivityInversionFit` object with the provided lens data and tracers.

If neither of these conditions are met, the function raises a `FittingException`, indicating that the properties of the tracers did not meet the requirements for either type of sensitivity fit."
8227,"def clear(self):
        """"""Clear description to default values""""""
        self._desc = {}
        for key, value in merge.DEFAULT_PROJECT.items():
            if key not in self._HIDDEN:
                self._desc[key] = type(value)()",python,Clear description to default values,50,"This code defines a method called `clear` in a Python class. The purpose of this method is to reset the instance's `_desc` attribute to its default values. It does this by first setting `_desc` to an empty dictionary. Then, it iterates over the items in `merge.DEFAULT_PROJECT`. If the key of an item is not in `_HIDDEN`, it adds the key to `_desc` with a default value, which is an instance of the type of the original value."
13641,"def _new_from_xml(cls, xmlnode):
        """"""Create a new `Item` object from an XML element.

        :Parameters:
            - `xmlnode`: the XML element.
        :Types:
            - `xmlnode`: `libxml2.xmlNode`

        :return: the object created.
        :returntype: `Item`
        """"""
        child = xmlnode.children
        fields = []
        while child:
            if child.type != ""element"" or child.ns().content != DATAFORM_NS:
                pass
            elif child.name == ""field"":
                fields.append(Field._new_from_xml(child))
            child = child.next
        return cls(fields)",python,"Create a new `Item` object from an XML element.

        :Parameters:
            - `xmlnode`: the XML element.
        :Types:
            - `xmlnode`: `libxml2.xmlNode`

        :return: the object created.
        :returntype: `Item`",138,"This code defines a method `_new_from_xml` that creates a new `Item` object from an XML element. It takes an XML node as an input parameter. It iterates over the children of the XML node, and if the child is an element and its namespace matches `DATAFORM_NS`, it checks if the child's name is ""field"". If it is, it creates a new `Field` object from the XML child and appends it to the `fields` list. The method then returns a new instance of the class it belongs to (`cls`), initialized with the `fields` list."
11025,"def convert_PSFLab_xz(data, x_step=0.5, z_step=0.5, normalize=False):
    """"""Process a 2D array (from PSFLab .mat file) containing a x-z PSF slice.

    The input data is the raw array saved by PSFLab. The returned array has
    the x axis cut in half (only positive x) to take advantage of the
    rotational symmetry around z. Pysical dimensions (`x_step` and `z_step)
    are also assigned.

    If `nomalize` is True the peak is normalized to 1.

    Returns:
    x, z: (1D array) the X and Z axis in pysical units
    hdata: (2D array) the PSF intesity
    izm: (float) the index of PSF max along z (axis 0) for x=0 (axis 1)
    """"""
    z_len, x_len = data.shape
    hdata = data[:, (x_len - 1) // 2:]
    x = np.arange(hdata.shape[1]) * x_step
    z = np.arange(-(z_len - 1) / 2, (z_len - 1) / 2 + 1) * z_step
    if normalize:
        hdata /= hdata.max()  # normalize to 1 at peak
    return x, z, hdata, hdata[:, 0].argmax()",python,"Process a 2D array (from PSFLab .mat file) containing a x-z PSF slice.

    The input data is the raw array saved by PSFLab. The returned array has
    the x axis cut in half (only positive x) to take advantage of the
    rotational symmetry around z. Pysical dimensions (`x_step` and `z_step)
    are also assigned.

    If `nomalize` is True the peak is normalized to 1.

    Returns:
    x, z: (1D array) the X and Z axis in pysical units
    hdata: (2D array) the PSF intesity
    izm: (float) the index of PSF max along z (axis 0) for x=0 (axis 1)",312,"This Python function, `convert_PSFLab_xz`, processes a 2D array from a PSFLab .mat file containing a x-z Point Spread Function (PSF) slice. The function takes four parameters: the data to be processed, the x and z step sizes, and a boolean flag to determine whether to normalize the data. 

The function first halves the x-axis of the input data to leverage the rotational symmetry around z. It then assigns physical dimensions to the x and z axes using the provided step sizes. 

If the `normalize` flag is set to True, the function normalizes the peak of the PSF intensity to 1. 

The function returns four values: the x and z axes in physical units, the processed PSF intensity, and the index of the PSF maximum along the z-axis for x=0."
10218,"def cf_dictionary_from_pairs(pairs):
        """"""
        Creates a CFDictionaryRef object from a list of 2-element tuples
        representing the key and value. Each key should be a CFStringRef and each
        value some sort of CF* type.

        :param pairs:
            A list of 2-element tuples

        :return:
            A CFDictionaryRef
        """"""

        length = len(pairs)
        keys = []
        values = []
        for pair in pairs:
            key, value = pair
            keys.append(key)
            values.append(value)
        return CoreFoundation.CFDictionaryCreate(
            CoreFoundation.kCFAllocatorDefault,
            keys,
            values,
            length,
            ffi.addressof(CoreFoundation.kCFTypeDictionaryKeyCallBacks),
            ffi.addressof(CoreFoundation.kCFTypeDictionaryValueCallBacks)
        )",python,"Creates a CFDictionaryRef object from a list of 2-element tuples
        representing the key and value. Each key should be a CFStringRef and each
        value some sort of CF* type.

        :param pairs:
            A list of 2-element tuples

        :return:
            A CFDictionaryRef",172,"This Python function, `cf_dictionary_from_pairs`, takes a list of 2-element tuples as input, where each tuple represents a key-value pair. It separates the keys and values into two separate lists. Then, it uses these lists along with the length of the pairs to create a CFDictionaryRef object using the `CFDictionaryCreate` function from the CoreFoundation library. This CFDictionaryRef object is then returned. The keys are expected to be CFStringRef type and the values can be any CF* type."
16525,"def read_pot_status(self):
        """"""Read the status of the digital pot. Firmware v18+ only.
        The return value is a dictionary containing the following as
        unsigned 8-bit integers: FanON, LaserON, FanDACVal, LaserDACVal.

        :rtype: dict

        :Example:

        >>> alpha.read_pot_status()
        {
            'LaserDACVal': 230,
            'FanDACVal': 255,
            'FanON': 0,
            'LaserON': 0
        }
        """"""
        # Send the command byte and wait 10 ms
        a = self.cnxn.xfer([0x13])[0]

        sleep(10e-3)

        # Build an array of the results
        res = []
        for i in range(4):
            res.append(self.cnxn.xfer([0x00])[0])

        sleep(0.1)

        return {
            'FanON':        res[0],
            'LaserON':      res[1],
            'FanDACVal':    res[2],
            'LaserDACVal':  res[3]
            }",python,"Read the status of the digital pot. Firmware v18+ only.
        The return value is a dictionary containing the following as
        unsigned 8-bit integers: FanON, LaserON, FanDACVal, LaserDACVal.

        :rtype: dict

        :Example:

        >>> alpha.read_pot_status()
        {
            'LaserDACVal': 230,
            'FanDACVal': 255,
            'FanON': 0,
            'LaserON': 0
        }",241,"This Python function, `read_pot_status`, is designed to read the status of a digital potentiometer. It first sends a command byte and waits for 10 milliseconds. Then, it builds an array by sending a zero byte four times, each time appending the response to the array. After a delay of 100 milliseconds, it returns a dictionary with the status of the fan and laser (whether they are on or off) and their respective DAC (Digital-to-Analog Converter) values. This function is intended to be used with firmware version 18 and above."
10858,"def curvature_matrix_from_blurred_mapping_matrix_jit(blurred_mapping_matrix, noise_map_1d, flist, iflist):
    """"""Compute the curvature matrix *F* from a blurred mapping matrix *f* and the 1D noise-map *\sigma* \
    (see Warren & Dye 2003).

    Parameters
    -----------
    blurred_mapping_matrix : ndarray
        The matrix representing the blurred mappings between sub-grid pixels and pixelization pixels.
    noise_map_1d : ndarray
        Flattened 1D array of the noise-map used by the inversion during the fit.
    flist : ndarray
        NumPy array of floats used to store mappings for efficienctly calculation.
    iflist : ndarray
        NumPy array of integers used to store mappings for efficienctly calculation.
    """"""
    curvature_matrix = np.zeros((blurred_mapping_matrix.shape[1], blurred_mapping_matrix.shape[1]))

    for image_index in range(blurred_mapping_matrix.shape[0]):
        index = 0
        for pixel_index in range(blurred_mapping_matrix.shape[1]):
            if blurred_mapping_matrix[image_index, pixel_index] > 0.0:
                flist[index] = blurred_mapping_matrix[image_index, pixel_index] / noise_map_1d[image_index]
                iflist[index] = pixel_index
                index += 1

        if index > 0:
            for i1 in range(index):
                for j1 in range(index):
                    ix = iflist[i1]
                    iy = iflist[j1]
                    curvature_matrix[ix, iy] += flist[i1] * flist[j1]

    for i in range(blurred_mapping_matrix.shape[1]):
        for j in range(blurred_mapping_matrix.shape[1]):
            curvature_matrix[i, j] = curvature_matrix[j, i]

    return curvature_matrix",python,"Compute the curvature matrix *F* from a blurred mapping matrix *f* and the 1D noise-map *\sigma* \
    (see Warren & Dye 2003).

    Parameters
    -----------
    blurred_mapping_matrix : ndarray
        The matrix representing the blurred mappings between sub-grid pixels and pixelization pixels.
    noise_map_1d : ndarray
        Flattened 1D array of the noise-map used by the inversion during the fit.
    flist : ndarray
        NumPy array of floats used to store mappings for efficienctly calculation.
    iflist : ndarray
        NumPy array of integers used to store mappings for efficienctly calculation.",388,"This Python function calculates the curvature matrix from a blurred mapping matrix and a 1D noise-map, as described in Warren & Dye 2003. The function takes four parameters: a 2D numpy array representing the blurred mappings between sub-grid pixels and pixelization pixels, a 1D numpy array representing the noise-map used by the inversion during the fit, and two additional numpy arrays (one float and one integer) used to store mappings for efficient calculation.

The function first initializes a zero matrix of the same size as the input matrix. It then iterates over each element in the blurred mapping matrix. If the element is greater than 0, it calculates a value by dividing the element by the corresponding value in the noise map and stores this value and the pixel index in the float and integer lists respectively.

Next, it calculates the elements of the curvature matrix by multiplying corresponding elements in the float list and adding the result to the current value in the curvature matrix. Finally, it makes the curvature matrix symmetric by copying the lower triangular part to the upper triangular part. The function returns the calculated curvature matrix."
19783,"def warmness(level=100, group=0):
    """""" Assumes level is out of 100 """"""
    if level not in range(0,101):
        raise Exception(""Warmness must be value between 0 and 100"")
    b = int(floor(level / 10.0)) #lights have 10 levels of warmness
    commands = list(coolest(group))
    for i in range(0, b):
        commands.append(COMMANDS['WARMER'])
    return tuple(commands)",python,Assumes level is out of 100,104,"This Python function, named 'warmness', takes two parameters: 'level' and 'group'. 'Level' is assumed to be out of 100 and defaults to 100 if not provided, while 'group' defaults to 0. The function first checks if the provided 'level' is within the range of 0 to 100, inclusive. If not, it raises an exception. It then calculates the 'warmness' level by dividing the 'level' by 10 and rounding down to the nearest integer. This is because the lights have 10 levels of warmness. It then creates a list of commands starting with the 'coolest' setting for the given 'group'. For each level of warmness, it appends the 'WARMER' command to the list. Finally, it returns the list of commands as a tuple."
6860,"def solve_T(self, P, V):
        r'''Method to calculate `T` from a specified `P` and `V` for the VDW
        EOS. Uses `a`, and `b`, obtained from the class's namespace.

        .. math::
            T =  \frac{1}{R V^{2}} \left(P V^{2} \left(V - b\right)
            + V a - a b\right)

        Parameters
        ----------
        P : float
            Pressure, [Pa]
        V : float
            Molar volume, [m^3/mol]

        Returns
        -------
        T : float
            Temperature, [K]
        '''
        return (P*V**2*(V - self.b) + V*self.a - self.a*self.b)/(R*V**2)",python,"r'''Method to calculate `T` from a specified `P` and `V` for the VDW
        EOS. Uses `a`, and `b`, obtained from the class's namespace.

        .. math::
            T =  \frac{1}{R V^{2}} \left(P V^{2} \left(V - b\right)
            + V a - a b\right)

        Parameters
        ----------
        P : float
            Pressure, [Pa]
        V : float
            Molar volume, [m^3/mol]

        Returns
        -------
        T : float
            Temperature, [K]",174,"This code defines a method named `solve_T` that calculates the temperature (`T`) from a specified pressure (`P`) and molar volume (`V`) using the Van der Waals equation of state (EOS). The method uses the constants `a` and `b` which are properties of the class this method belongs to. The formula used for the calculation is provided in the docstring. The method takes two parameters, pressure and molar volume, and returns the calculated temperature."
12413,"def bls_stats_singleperiod(times, mags, errs, period,
                           magsarefluxes=False,
                           sigclip=10.0,
                           perioddeltapercent=10,
                           nphasebins=200,
                           mintransitduration=0.01,
                           maxtransitduration=0.4,
                           ingressdurationfraction=0.1,
                           verbose=True):
    '''This calculates the SNR, depth, duration, a refit period, and time of
    center-transit for a single period.

    The equation used for SNR is::

        SNR = (transit model depth / RMS of LC with transit model subtracted)
              * sqrt(number of points in transit)

    NOTE: you should set the kwargs `sigclip`, `nphasebins`,
    `mintransitduration`, `maxtransitduration` to what you used for an initial
    BLS run to detect transits in the input light curve to match those input
    conditions.

    Parameters
    ----------

    times,mags,errs : np.array
        These contain the magnitude/flux time-series and any associated errors.

    period : float
        The period to search around and refit the transits. This will be used to
        calculate the start and end periods of a rerun of BLS to calculate the
        stats.

    magsarefluxes : bool
        Set to True if the input measurements in `mags` are actually fluxes and
        not magnitudes.

    sigclip : float or int or sequence of two floats/ints or None
        If a single float or int, a symmetric sigma-clip will be performed using
        the number provided as the sigma-multiplier to cut out from the input
        time-series.

        If a list of two ints/floats is provided, the function will perform an
        'asymmetric' sigma-clip. The first element in this list is the sigma
        value to use for fainter flux/mag values; the second element in this
        list is the sigma value to use for brighter flux/mag values. For
        example, `sigclip=[10., 3.]`, will sigclip out greater than 10-sigma
        dimmings and greater than 3-sigma brightenings. Here the meaning of
        ""dimming"" and ""brightening"" is set by *physics* (not the magnitude
        system), which is why the `magsarefluxes` kwarg must be correctly set.

        If `sigclip` is None, no sigma-clipping will be performed, and the
        time-series (with non-finite elems removed) will be passed through to
        the output.

    perioddeltapercent : float
        The fraction of the period provided to use to search around this
        value. This is a percentage. The period range searched will then be::

            [period - (perioddeltapercent/100.0)*period,
             period + (perioddeltapercent/100.0)*period]

    nphasebins : int
        The number of phase bins to use in the BLS run.

    mintransitduration : float
        The minimum transit duration in phase to consider.

    maxtransitduration : float
        The maximum transit duration to consider.

    ingressdurationfraction : float
        The fraction of the transit duration to use to generate an initial value
        of the transit ingress duration for the BLS model refit. This will be
        fit by this function.

    verbose : bool
        If True, will indicate progress and any problems encountered.

    Returns
    -------

    dict
        A dict of the following form is returned::

            {'period': the refit best period,
             'epoch': the refit epoch (i.e. mid-transit time),
             'snr':the SNR of the transit,
             'transitdepth':the depth of the transit,
             'transitduration':the duration of the transit,
             'nphasebins':the input value of nphasebins,
             'transingressbin':the phase bin containing transit ingress,
             'transegressbin':the phase bin containing transit egress,
             'blsmodel':the full BLS model used along with its parameters,
             'subtractedmags':BLS model - phased light curve,
             'phasedmags':the phase light curve,
             'phases': the phase values}

    '''

    # get rid of nans first and sigclip
    stimes, smags, serrs = sigclip_magseries(times,
                                             mags,
                                             errs,
                                             magsarefluxes=magsarefluxes,
                                             sigclip=sigclip)


    # make sure there are enough points to calculate a spectrum
    if len(stimes) > 9 and len(smags) > 9 and len(serrs) > 9:

        # get the period interval
        startp = period - perioddeltapercent*period/100.0

        if startp < 0:
            startp = period

        endp = period + perioddeltapercent*period/100.0

        # rerun BLS in serial mode around the specified period to get the
        # transit depth, duration, ingress and egress bins
        blsres = bls_serial_pfind(stimes, smags, serrs,
                                  verbose=verbose,
                                  startp=startp,
                                  endp=endp,
                                  nphasebins=nphasebins,
                                  mintransitduration=mintransitduration,
                                  maxtransitduration=maxtransitduration,
                                  magsarefluxes=magsarefluxes,
                                  get_stats=False,
                                  sigclip=None)

        if (not blsres or
            'blsresult' not in blsres or
            blsres['blsresult'] is None):
            LOGERROR(""BLS failed during a period-search ""
                     ""performed around the input best period: %.6f. ""
                     ""Can't continue. "" % period)
            return None

        thistransdepth = blsres['blsresult']['transdepth']
        thistransduration = blsres['blsresult']['transduration']
        thisbestperiod = blsres['bestperiod']
        thistransingressbin = blsres['blsresult']['transingressbin']
        thistransegressbin = blsres['blsresult']['transegressbin']
        thisnphasebins = nphasebins

        stats = _get_bls_stats(stimes,
                               smags,
                               serrs,
                               thistransdepth,
                               thistransduration,
                               ingressdurationfraction,
                               nphasebins,
                               thistransingressbin,
                               thistransegressbin,
                               thisbestperiod,
                               thisnphasebins,
                               magsarefluxes=magsarefluxes,
                               verbose=verbose)

        return stats


    # if there aren't enough points in the mag series, bail out
    else:

        LOGERROR('no good detections for these times and mags, skipping...')
        return None",python,"This calculates the SNR, depth, duration, a refit period, and time of
    center-transit for a single period.

    The equation used for SNR is::

        SNR = (transit model depth / RMS of LC with transit model subtracted)
              * sqrt(number of points in transit)

    NOTE: you should set the kwargs `sigclip`, `nphasebins`,
    `mintransitduration`, `maxtransitduration` to what you used for an initial
    BLS run to detect transits in the input light curve to match those input
    conditions.

    Parameters
    ----------

    times,mags,errs : np.array
        These contain the magnitude/flux time-series and any associated errors.

    period : float
        The period to search around and refit the transits. This will be used to
        calculate the start and end periods of a rerun of BLS to calculate the
        stats.

    magsarefluxes : bool
        Set to True if the input measurements in `mags` are actually fluxes and
        not magnitudes.

    sigclip : float or int or sequence of two floats/ints or None
        If a single float or int, a symmetric sigma-clip will be performed using
        the number provided as the sigma-multiplier to cut out from the input
        time-series.

        If a list of two ints/floats is provided, the function will perform an
        'asymmetric' sigma-clip. The first element in this list is the sigma
        value to use for fainter flux/mag values; the second element in this
        list is the sigma value to use for brighter flux/mag values. For
        example, `sigclip=[10., 3.]`, will sigclip out greater than 10-sigma
        dimmings and greater than 3-sigma brightenings. Here the meaning of
        ""dimming"" and ""brightening"" is set by *physics* (not the magnitude
        system), which is why the `magsarefluxes` kwarg must be correctly set.

        If `sigclip` is None, no sigma-clipping will be performed, and the
        time-series (with non-finite elems removed) will be passed through to
        the output.

    perioddeltapercent : float
        The fraction of the period provided to use to search around this
        value. This is a percentage. The period range searched will then be::

            [period - (perioddeltapercent/100.0)*period,
             period + (perioddeltapercent/100.0)*period]

    nphasebins : int
        The number of phase bins to use in the BLS run.

    mintransitduration : float
        The minimum transit duration in phase to consider.

    maxtransitduration : float
        The maximum transit duration to consider.

    ingressdurationfraction : float
        The fraction of the transit duration to use to generate an initial value
        of the transit ingress duration for the BLS model refit. This will be
        fit by this function.

    verbose : bool
        If True, will indicate progress and any problems encountered.

    Returns
    -------

    dict
        A dict of the following form is returned::

            {'period': the refit best period,
             'epoch': the refit epoch (i.e. mid-transit time),
             'snr':the SNR of the transit,
             'transitdepth':the depth of the transit,
             'transitduration':the duration of the transit,
             'nphasebins':the input value of nphasebins,
             'transingressbin':the phase bin containing transit ingress,
             'transegressbin':the phase bin containing transit egress,
             'blsmodel':the full BLS model used along with its parameters,
             'subtractedmags':BLS model - phased light curve,
             'phasedmags':the phase light curve,
             'phases': the phase values}",1503,"This Python function, `bls_stats_singleperiod`, is used to calculate various statistics related to the transit of a celestial body. It takes in parameters such as time, magnitude, errors, period, and several optional parameters to customize the calculation. 

The function first performs a sigma-clipping operation on the input time-series data to remove outliers. It then checks if there are enough data points to proceed. If there are, it calculates the period interval and reruns the Box Least Squares (BLS) algorithm around the specified period to get the transit depth, duration, ingress and egress bins. 

The function then calls another function `_get_bls_stats` to calculate the Signal-to-Noise Ratio (SNR), depth, duration, a refit period, and time of center-transit for a single period. 

If there aren't enough data points, the function logs an error message and returns None. The function returns a dictionary containing the calculated statistics if successful."
4434,"def _create_task_log_info(self, task_id, fail_mode=None):
        """"""
        Create the dictionary that will be included in the log.
        """"""

        info_to_monitor = ['func_name', 'fn_hash', 'memoize', 'checkpoint', 'fail_count',
                           'fail_history', 'status', 'id', 'time_submitted', 'time_returned', 'executor']

        task_log_info = {""task_"" + k: self.tasks[task_id][k] for k in info_to_monitor}
        task_log_info['run_id'] = self.run_id
        task_log_info['timestamp'] = datetime.datetime.now()
        task_log_info['task_status_name'] = self.tasks[task_id]['status'].name
        task_log_info['tasks_failed_count'] = self.tasks_failed_count
        task_log_info['tasks_completed_count'] = self.tasks_completed_count
        task_log_info['task_inputs'] = str(self.tasks[task_id]['kwargs'].get('inputs', None))
        task_log_info['task_outputs'] = str(self.tasks[task_id]['kwargs'].get('outputs', None))
        task_log_info['task_stdin'] = self.tasks[task_id]['kwargs'].get('stdin', None)
        task_log_info['task_stdout'] = self.tasks[task_id]['kwargs'].get('stdout', None)
        task_log_info['task_depends'] = None
        if self.tasks[task_id]['depends'] is not None:
            task_log_info['task_depends'] = "","".join([str(t._tid) for t in self.tasks[task_id]['depends']])
        task_log_info['task_elapsed_time'] = None
        if self.tasks[task_id]['time_returned'] is not None:
            task_log_info['task_elapsed_time'] = (self.tasks[task_id]['time_returned'] -
                                                  self.tasks[task_id]['time_submitted']).total_seconds()
        if fail_mode is not None:
            task_log_info['task_fail_mode'] = fail_mode
        return task_log_info",python,Create the dictionary that will be included in the log.,421,"This Python function, `_create_task_log_info`, is used to create a dictionary that contains log information for a specific task. The task is identified by its `task_id`. The function collects various pieces of information about the task, such as its function name, status, submission and return times, and any input or output it has. It also includes information about the task's dependencies and the time it took to complete. If the task has failed, the function records the failure mode. The resulting dictionary is then returned. This function is likely part of a larger system that manages and monitors the execution of tasks."
12552,"def spend_key(self):
        """"""
        Returns private spend key. None if wallet is view-only.

        :rtype: str or None
        """"""
        key = self._backend.spend_key()
        if key == numbers.EMPTY_KEY:
            return None
        return key",python,"Returns private spend key. None if wallet is view-only.

        :rtype: str or None",55,"This code defines a method called `spend_key` in a class. The method is used to get the private spend key from the backend. If the wallet is view-only, the backend will return an empty key, and in this case, the method will return None. Otherwise, it will return the private spend key."
2208,"def _fire(self, layers, the_plot):
    """"""Launches a new bolt from a random Marauder.""""""
    # We don't fire if another Marauder fired a bolt just now.
    if the_plot.get('last_marauder_shot') == the_plot.frame: return
    the_plot['last_marauder_shot'] = the_plot.frame
    # Which Marauder should fire the laser bolt?
    col = np.random.choice(np.nonzero(layers['X'].sum(axis=0))[0])
    row = np.nonzero(layers['X'][:, col])[0][-1] + 1
    # Move ourselves just below that Marauder.
    self._teleport((row, col))",python,Launches a new bolt from a random Marauder.,149,"This code defines a function `_fire` that simulates the firing of a bolt from a random 'Marauder'. The function takes in two parameters: `layers`, which likely represents the game state, and `the_plot`, which likely represents the current frame or time step of the game. 

The function first checks if a Marauder has fired a bolt in the current frame. If so, it exits the function early. If not, it records the current frame as the time of the last shot. 

Next, it randomly selects a Marauder (represented by a column in the `layers` matrix) that has not yet fired. It then finds the row just below that Marauder, which is where the bolt will appear. 

Finally, it calls the `_teleport` function to move the bolt to the selected location."
18225,"def init_app(self, app):
        """"""Flask application initialization.""""""
        self.init_config(app)
        app.register_blueprint(blueprint)
        app.extensions['invenio-groups'] = self",python,Flask application initialization.,39,"This code snippet is a method for initializing a Flask application. It first initializes the configuration of the application by calling the `init_config` method. Then, it registers a blueprint to the application, which is a template for generating views and organizing application functions. Finally, it adds itself to the application's extensions dictionary under the key 'invenio-groups'. This allows the extension to be accessible from the application object."
9489,"def _parse_local_mount_uri(self, raw_uri):
    """"""Return a valid docker_path for a local file path.""""""
    raw_uri = directory_fmt(raw_uri)
    _, docker_path = _local_uri_rewriter(raw_uri)
    local_path = docker_path[len('file'):]
    docker_uri = os.path.join(self._relative_path, docker_path)
    return local_path, docker_uri",python,Return a valid docker_path for a local file path.,79,"This code defines a function `_parse_local_mount_uri` that takes a raw URI (Uniform Resource Identifier) as an argument and returns a valid docker path for a local file path. The function first formats the raw URI into a directory format. Then, it uses the `_local_uri_rewriter` function to rewrite the URI and extract the docker path. It also extracts the local path from the docker path by removing the initial 'file' string. Finally, it constructs the docker URI by joining the relative path with the docker path and returns both the local path and the docker URI."
11042,"def add_emission(self, chunksize=2**19, comp_filter=default_compression,
                     overwrite=False, params=dict(), chunkslice='bytes'):
        """"""Add the `emission` array in '/trajectories'.
        """"""
        nparams = self.numeric_params
        num_particles = nparams['np']

        return self.add_trajectory('emission', shape=(num_particles, 0),
                                   overwrite=overwrite, chunksize=chunksize,
                                   comp_filter=comp_filter,
                                   atom=tables.Float32Atom(),
                                   title='Emission trace of each particle',
                                   params=params)",python,Add the `emission` array in '/trajectories'.,122,"This code defines a method named `add_emission` in a class. The method is used to add an `emission` array to the '/trajectories' directory. The shape of the array is determined by the number of particles, which is retrieved from the numeric parameters of the class. The method also allows for customization of the chunk size, compression filter, whether to overwrite existing data, and other parameters. The data type of the array elements is set to be 32-bit floating point numbers. The method returns the result of the `add_trajectory` method call."
5614,"def sectorPerformanceDF(token='', version=''):
    '''This returns an array of each sector and performance for the current trading day. Performance is based on each sector ETF.

    https://iexcloud.io/docs/api/#sector-performance
    8am-5pm ET Mon-Fri

    Args:
        token (string); Access token
        version (string); API version

    Returns:
        DataFrame: result
    '''
    df = pd.DataFrame(sectorPerformance(token, version))
    _toDatetime(df)
    _reindex(df, 'name')
    return df",python,"This returns an array of each sector and performance for the current trading day. Performance is based on each sector ETF.

    https://iexcloud.io/docs/api/#sector-performance
    8am-5pm ET Mon-Fri

    Args:
        token (string); Access token
        version (string); API version

    Returns:
        DataFrame: result",117,"This Python function, `sectorPerformanceDF`, retrieves the performance of various sectors for the current trading day. The performance is based on each sector's Exchange Traded Fund (ETF). The function takes two arguments: an access token and an API version. It then calls the `sectorPerformance` function with these arguments and converts the result into a pandas DataFrame. The `_toDatetime` function is called to convert any date/time data to datetime objects, and `_reindex` is used to reindex the DataFrame based on the 'name' column. The function returns this DataFrame. The data is sourced from the IEX Cloud API, which is available from 8am to 5pm ET, Monday to Friday."
16830,"def update(self, instance, validated_data):
        """"""
        Update the instance the serializer is bound to.

        Args:
            instance:
                The instance the serializer is bound to.
            validated_data:
                The data to update the serializer with.

        Returns:
            The updated instance.
        """"""
        is_primary = validated_data.pop(""is_primary"", False)

        instance = super(EmailSerializer, self).update(
            instance, validated_data
        )

        if is_primary:
            instance.set_primary()

        return instance",python,"Update the instance the serializer is bound to.

        Args:
            instance:
                The instance the serializer is bound to.
            validated_data:
                The data to update the serializer with.

        Returns:
            The updated instance.",105,"This code defines an `update` method for a serializer, likely in Django REST framework. The method takes two arguments: `instance` and `validated_data`. `instance` is the current object that the serializer is bound to, and `validated_data` is the new data that will be used to update the `instance`. 

The method first checks if `is_primary` is included in `validated_data` and removes it, defaulting to `False` if it's not present. 

Then, it calls the parent class's `update` method to perform the actual update of the `instance` with the `validated_data`. 

If `is_primary` was `True`, it calls a method `set_primary` on the `instance`, presumably to set this instance as the primary one. 

Finally, it returns the updated `instance`."
1526,"def getNextRecordDict(self):
    """"""Returns next available data record from the storage as a dict, with the
    keys being the field names. This also adds in some meta fields:

      - ``_category``: The value from the category field (if any)
      - ``_reset``: True if the reset field was True (if any)
      - ``_sequenceId``: the value from the sequenceId field (if any)

    """"""

    values = self.getNextRecord()
    if values is None:
      return None

    if not values:
      return dict()

    if self._modelRecordEncoder is None:
      self._modelRecordEncoder = ModelRecordEncoder(
        fields=self.getFields(),
        aggregationPeriod=self.getAggregationMonthsAndSeconds())

    return self._modelRecordEncoder.encode(values)",python,"Returns next available data record from the storage as a dict, with the
    keys being the field names. This also adds in some meta fields:

      - ``_category``: The value from the category field (if any)
      - ``_reset``: True if the reset field was True (if any)
      - ``_sequenceId``: the value from the sequenceId field (if any)",168,"This function, `getNextRecordDict`, retrieves the next available data record from a storage system and returns it as a dictionary, where the keys are the field names. It also adds some metadata fields to the dictionary, such as `_category`, `_reset`, and `_sequenceId`. If there are no more records to retrieve, the function will return `None`. If the retrieved record is empty, it will return an empty dictionary. If the `_modelRecordEncoder` is not initialized, it will be set up with the fields and aggregation period. Finally, the function encodes the values using the `_modelRecordEncoder` and returns the result."
2560,"def run(cl_args, compo_type):
  """""" run command """"""
  cluster, role, env = cl_args['cluster'], cl_args['role'], cl_args['environ']
  topology = cl_args['topology-name']
  spouts_only, bolts_only = cl_args['spout'], cl_args['bolt']
  try:
    components = tracker_access.get_logical_plan(cluster, env, topology, role)
    topo_info = tracker_access.get_topology_info(cluster, env, topology, role)
    table, header = to_table(components, topo_info)
    if spouts_only == bolts_only:
      print(tabulate(table, headers=header))
    elif spouts_only:
      table, header = filter_spouts(table, header)
      print(tabulate(table, headers=header))
    else:
      table, header = filter_bolts(table, header)
      print(tabulate(table, headers=header))
    return True
  except:
    Log.error(""Fail to connect to tracker: \'%s\'"", cl_args[""tracker_url""])
    return False",python,run command,218,"This code defines a function named `run` that takes two arguments: `cl_args` (a dictionary of command line arguments) and `comp_type`. It extracts various parameters from `cl_args` and uses them to retrieve a logical plan and topology info from a tracker. It then converts these into a table format. If the command line arguments specify only spouts or only bolts, it filters the table accordingly. The table is then printed. If any errors occur during this process, it logs an error message and returns False. Otherwise, it returns True."
440,"def plot_rolling_returns(returns,
                         factor_returns=None,
                         live_start_date=None,
                         logy=False,
                         cone_std=None,
                         legend_loc='best',
                         volatility_match=False,
                         cone_function=timeseries.forecast_cone_bootstrap,
                         ax=None, **kwargs):
    """"""
    Plots cumulative rolling returns versus some benchmarks'.

    Backtest returns are in green, and out-of-sample (live trading)
    returns are in red.

    Additionally, a non-parametric cone plot may be added to the
    out-of-sample returns region.

    Parameters
    ----------
    returns : pd.Series
        Daily returns of the strategy, noncumulative.
         - See full explanation in tears.create_full_tear_sheet.
    factor_returns : pd.Series, optional
        Daily noncumulative returns of the benchmark factor to which betas are
        computed. Usually a benchmark such as market returns.
         - This is in the same style as returns.
    live_start_date : datetime, optional
        The date when the strategy began live trading, after
        its backtest period. This date should be normalized.
    logy : bool, optional
        Whether to log-scale the y-axis.
    cone_std : float, or tuple, optional
        If float, The standard deviation to use for the cone plots.
        If tuple, Tuple of standard deviation values to use for the cone plots
         - See timeseries.forecast_cone_bounds for more details.
    legend_loc : matplotlib.loc, optional
        The location of the legend on the plot.
    volatility_match : bool, optional
        Whether to normalize the volatility of the returns to those of the
        benchmark returns. This helps compare strategies with different
        volatilities. Requires passing of benchmark_rets.
    cone_function : function, optional
        Function to use when generating forecast probability cone.
        The function signiture must follow the form:
        def cone(in_sample_returns (pd.Series),
                 days_to_project_forward (int),
                 cone_std= (float, or tuple),
                 starting_value= (int, or float))
        See timeseries.forecast_cone_bootstrap for an example.
    ax : matplotlib.Axes, optional
        Axes upon which to plot.
    **kwargs, optional
        Passed to plotting function.

    Returns
    -------
    ax : matplotlib.Axes
        The axes that were plotted on.
    """"""

    if ax is None:
        ax = plt.gca()

    ax.set_xlabel('')
    ax.set_ylabel('Cumulative returns')
    ax.set_yscale('log' if logy else 'linear')

    if volatility_match and factor_returns is None:
        raise ValueError('volatility_match requires passing of '
                         'factor_returns.')
    elif volatility_match and factor_returns is not None:
        bmark_vol = factor_returns.loc[returns.index].std()
        returns = (returns / returns.std()) * bmark_vol

    cum_rets = ep.cum_returns(returns, 1.0)

    y_axis_formatter = FuncFormatter(utils.two_dec_places)
    ax.yaxis.set_major_formatter(FuncFormatter(y_axis_formatter))

    if factor_returns is not None:
        cum_factor_returns = ep.cum_returns(
            factor_returns[cum_rets.index], 1.0)
        cum_factor_returns.plot(lw=2, color='gray',
                                label=factor_returns.name, alpha=0.60,
                                ax=ax, **kwargs)

    if live_start_date is not None:
        live_start_date = ep.utils.get_utc_timestamp(live_start_date)
        is_cum_returns = cum_rets.loc[cum_rets.index < live_start_date]
        oos_cum_returns = cum_rets.loc[cum_rets.index >= live_start_date]
    else:
        is_cum_returns = cum_rets
        oos_cum_returns = pd.Series([])

    is_cum_returns.plot(lw=3, color='forestgreen', alpha=0.6,
                        label='Backtest', ax=ax, **kwargs)

    if len(oos_cum_returns) > 0:
        oos_cum_returns.plot(lw=4, color='red', alpha=0.6,
                             label='Live', ax=ax, **kwargs)

        if cone_std is not None:
            if isinstance(cone_std, (float, int)):
                cone_std = [cone_std]

            is_returns = returns.loc[returns.index < live_start_date]
            cone_bounds = cone_function(
                is_returns,
                len(oos_cum_returns),
                cone_std=cone_std,
                starting_value=is_cum_returns[-1])

            cone_bounds = cone_bounds.set_index(oos_cum_returns.index)
            for std in cone_std:
                ax.fill_between(cone_bounds.index,
                                cone_bounds[float(std)],
                                cone_bounds[float(-std)],
                                color='steelblue', alpha=0.5)

    if legend_loc is not None:
        ax.legend(loc=legend_loc, frameon=True, framealpha=0.5)
    ax.axhline(1.0, linestyle='--', color='black', lw=2)

    return ax",python,"Plots cumulative rolling returns versus some benchmarks'.

    Backtest returns are in green, and out-of-sample (live trading)
    returns are in red.

    Additionally, a non-parametric cone plot may be added to the
    out-of-sample returns region.

    Parameters
    ----------
    returns : pd.Series
        Daily returns of the strategy, noncumulative.
         - See full explanation in tears.create_full_tear_sheet.
    factor_returns : pd.Series, optional
        Daily noncumulative returns of the benchmark factor to which betas are
        computed. Usually a benchmark such as market returns.
         - This is in the same style as returns.
    live_start_date : datetime, optional
        The date when the strategy began live trading, after
        its backtest period. This date should be normalized.
    logy : bool, optional
        Whether to log-scale the y-axis.
    cone_std : float, or tuple, optional
        If float, The standard deviation to use for the cone plots.
        If tuple, Tuple of standard deviation values to use for the cone plots
         - See timeseries.forecast_cone_bounds for more details.
    legend_loc : matplotlib.loc, optional
        The location of the legend on the plot.
    volatility_match : bool, optional
        Whether to normalize the volatility of the returns to those of the
        benchmark returns. This helps compare strategies with different
        volatilities. Requires passing of benchmark_rets.
    cone_function : function, optional
        Function to use when generating forecast probability cone.
        The function signiture must follow the form:
        def cone(in_sample_returns (pd.Series),
                 days_to_project_forward (int),
                 cone_std= (float, or tuple),
                 starting_value= (int, or float))
        See timeseries.forecast_cone_bootstrap for an example.
    ax : matplotlib.Axes, optional
        Axes upon which to plot.
    **kwargs, optional
        Passed to plotting function.

    Returns
    -------
    ax : matplotlib.Axes
        The axes that were plotted on.",1052,"This Python function, `plot_rolling_returns`, is used to plot the cumulative rolling returns of a trading strategy against some benchmarks. The function takes several parameters including the daily returns of the strategy, the daily returns of the benchmark, the date when live trading started, whether to log-scale the y-axis, the standard deviation for the cone plots, the location of the legend on the plot, whether to normalize the volatility of the returns, the function to use when generating forecast probability cone, and the axes upon which to plot.

The function first sets the labels and scale for the axes. If volatility matching is enabled, it normalizes the volatility of the strategy returns to that of the benchmark returns. It then calculates the cumulative returns of the strategy and the benchmark, and plots them. If a live start date is provided, it separates the returns into in-sample (backtest) and out-of-sample (live trading) returns, and plots them in different colors. If a standard deviation for the cone plots is provided, it generates a forecast probability cone and adds it to the plot. Finally, it adds a legend to the plot and returns the axes that were plotted on."
10627,"def plot_points(points_arcsec, array, units, kpc_per_arcsec, pointsize, zoom_offset_arcsec):
    """"""Plot a set of points over the array of data on the figure.

    Parameters
    -----------
    positions : [[]]
        Lists of (y,x) coordinates on the image which are plotted as colored dots, to highlight specific pixels.
    array : data.array.scaled_array.ScaledArray
        The 2D array of data which is plotted.
    units : str
        The units of the y / x axis of the plots, in arc-seconds ('arcsec') or kiloparsecs ('kpc').
    kpc_per_arcsec : float or None
        The conversion factor between arc-seconds and kiloparsecs, required to plot the units in kpc.
    pointsize : int
        The size of the points plotted to show the input positions.
    """"""
    if points_arcsec is not None:
        points_arcsec = list(map(lambda position_set: np.asarray(position_set), points_arcsec))
        point_colors = itertools.cycle([""m"", ""y"", ""r"", ""w"", ""c"", ""b"", ""g"", ""k""])
        for point_set_arcsec in points_arcsec:

            if zoom_offset_arcsec is not None:
                point_set_arcsec -= zoom_offset_arcsec

            point_set_units = convert_grid_units(array=array, grid_arcsec=point_set_arcsec, units=units,
                                                 kpc_per_arcsec=kpc_per_arcsec)
            plt.scatter(y=point_set_units[:,0], x=point_set_units[:,1], color=next(point_colors), s=pointsize)",python,"Plot a set of points over the array of data on the figure.

    Parameters
    -----------
    positions : [[]]
        Lists of (y,x) coordinates on the image which are plotted as colored dots, to highlight specific pixels.
    array : data.array.scaled_array.ScaledArray
        The 2D array of data which is plotted.
    units : str
        The units of the y / x axis of the plots, in arc-seconds ('arcsec') or kiloparsecs ('kpc').
    kpc_per_arcsec : float or None
        The conversion factor between arc-seconds and kiloparsecs, required to plot the units in kpc.
    pointsize : int
        The size of the points plotted to show the input positions.",346,"This Python function, `plot_points`, is used to plot a set of points over a 2D array of data. The points are represented by their (y,x) coordinates. The function takes several parameters including the points to be plotted, the 2D array of data, the units of the y/x axis, the conversion factor between arc-seconds and kiloparsecs, the size of the points, and an optional zoom offset. 

The function first checks if the points are not None, then converts them to numpy arrays. It cycles through a set of colors for the points. For each set of points, if a zoom offset is provided, it is subtracted from the points. Then, the points are converted to the specified units using the `convert_grid_units` function. Finally, the points are plotted on a scatter plot with the specified color and size."
9614,"async def write(self, data):
        """"""
        :py:func:`asyncio.coroutine`

        :py:meth:`aioftp.StreamIO.write` proxy
        """"""
        await self.wait(""write"")
        start = _now()
        await super().write(data)
        self.append(""write"", data, start)",python,":py:func:`asyncio.coroutine`

        :py:meth:`aioftp.StreamIO.write` proxy",67,"This is an asynchronous function named 'write' in Python, which is used to write data. It first waits for the ""write"" event to be ready. The '_now()' function is called to get the current time before the writing operation begins. The 'super().write(data)' line calls a write method from a parent class to perform the actual writing of the data. After the data is written, the 'append' method is called to record the ""write"" operation, the data, and the start time. This function is a part of the 'aioftp' library, which is used for FTP (File Transfer Protocol) operations in an asynchronous manner."
17966,"def fork(self,name):
        '''
        Create fork and store it in current instance
        '''
        fork=deepcopy(self)
        self[name]=fork
        return fork",python,Create fork and store it in current instance,35,"This code defines a method called 'fork' in a class. The method takes a 'name' as an argument. It creates a deep copy of the current instance of the object, which means it duplicates not just the object but also the objects referenced by the original object. This new copy is then stored in the original object under the key specified by 'name'. The new copy is then returned by the method. This method is typically used to create a new branch or version of an object, hence the name 'fork'."
11293,"def _apply_final_rules(self, phonetic, final_rules, language_arg, strip):
        """"""Apply a set of final rules to the phonetic encoding.

        Parameters
        ----------
        phonetic : str
            The term to which to apply the final rules
        final_rules : tuple
            The set of final phonetic transform regexps
        language_arg : int
            An integer representing the target language of the phonetic
            encoding
        strip : bool
            Flag to indicate whether to normalize the language attributes

        Returns
        -------
        str
            A Beider-Morse phonetic code

        """"""
        # optimization to save time
        if not final_rules:
            return phonetic

        # expand the result
        phonetic = self._expand_alternates(phonetic)
        phonetic_array = phonetic.split('|')

        for k in range(len(phonetic_array)):
            phonetic = phonetic_array[k]
            phonetic2 = ''
            phoneticx = self._normalize_lang_attrs(phonetic, True)

            i = 0
            while i < len(phonetic):
                found = False

                if phonetic[i] == '[':  # skip over language attribute
                    attrib_start = i
                    i += 1
                    while True:
                        if phonetic[i] == ']':
                            i += 1
                            phonetic2 += phonetic[attrib_start:i]
                            break
                        i += 1
                    continue

                for rule in final_rules:
                    pattern = rule[_PATTERN_POS]
                    pattern_length = len(pattern)
                    lcontext = rule[_LCONTEXT_POS]
                    rcontext = rule[_RCONTEXT_POS]

                    right = '^' + rcontext
                    left = lcontext + '$'

                    # check to see if next sequence in phonetic matches the
                    # string in the rule
                    if (pattern_length > len(phoneticx) - i) or phoneticx[
                        i : i + pattern_length
                    ] != pattern:
                        continue

                    # check that right context is satisfied
                    if rcontext != '':
                        if not search(right, phoneticx[i + pattern_length :]):
                            continue

                    # check that left context is satisfied
                    if lcontext != '':
                        if not search(left, phoneticx[:i]):
                            continue

                    # check for incompatible attributes
                    candidate = self._apply_rule_if_compat(
                        phonetic2, rule[_PHONETIC_POS], language_arg
                    )
                    # The below condition shouldn't ever be false
                    if candidate is not None:  # pragma: no branch
                        phonetic2 = candidate
                        found = True
                        break

                if not found:
                    # character in name for which there is no substitution in
                    # the table
                    phonetic2 += phonetic[i]
                    pattern_length = 1

                i += pattern_length

            phonetic_array[k] = self._expand_alternates(phonetic2)

        phonetic = '|'.join(phonetic_array)
        if strip:
            phonetic = self._normalize_lang_attrs(phonetic, True)

        if '|' in phonetic:
            phonetic = '(' + self._remove_dupes(phonetic) + ')'

        return phonetic",python,"Apply a set of final rules to the phonetic encoding.

        Parameters
        ----------
        phonetic : str
            The term to which to apply the final rules
        final_rules : tuple
            The set of final phonetic transform regexps
        language_arg : int
            An integer representing the target language of the phonetic
            encoding
        strip : bool
            Flag to indicate whether to normalize the language attributes

        Returns
        -------
        str
            A Beider-Morse phonetic code",685,"This Python function, `_apply_final_rules`, is part of a phonetic encoding system, specifically the Beider-Morse Phonetic Matching (BMPM) system. It applies a set of final transformation rules to a phonetic string. 

The function takes four parameters: `phonetic` (the string to be transformed), `final_rules` (a tuple of transformation rules), `language_arg` (an integer representing the target language), and `strip` (a boolean flag indicating whether to normalize language attributes). 

The function first checks if there are any final rules to apply. If not, it returns the original phonetic string. If there are rules, it expands any alternate phonetic encodings and splits the string into an array. 

It then iterates over each phonetic string in the array, applying the transformation rules. It checks for matching patterns in the phonetic string and applies the corresponding transformation if the left and right context conditions are satisfied. If no rule matches, it simply appends the character to the transformed string. 

After applying all rules, it re-expands any alternates and joins the array back into a string. If the `strip` flag is set, it normalizes the language attributes. 

Finally, if the phonetic string contains alternates, it removes duplicates and wraps the string in parentheses before returning it."
12067,"def check_ok(self):
        """"""
        Ensures all tests have passed for this branch.

        This should be called before deployment, to prevent accidental deployment of code
        that hasn't passed automated testing.
        """"""
        import requests

        if not self.env.check_ok:
            return

        # Find current git branch.
        branch_name = self._local('git rev-parse --abbrev-ref HEAD', capture=True).strip()

        check_ok_paths = self.env.check_ok_paths or {}

        if branch_name in check_ok_paths:
            check = check_ok_paths[branch_name]
            if 'username' in check:
                auth = (check['username'], check['password'])
            else:
                auth = None
            ret = requests.get(check['url'], auth=auth)
            passed = check['text'] in ret.content
            assert passed, 'Check failed: %s' % check['url']",python,"Ensures all tests have passed for this branch.

        This should be called before deployment, to prevent accidental deployment of code
        that hasn't passed automated testing.",185,"This Python function, `check_ok`, is designed to ensure that all tests have passed for the current Git branch before deployment. It prevents accidental deployment of code that hasn't passed automated testing. 

The function first checks if the environment is okay for testing. If not, it returns immediately. If it is okay, it retrieves the name of the current Git branch. 

It then checks if the branch name exists in the `check_ok_paths` dictionary. If it does, it retrieves the check details for that branch. If the check details include a username, it uses that username and password for authentication. Otherwise, no authentication is used. 

A GET request is then sent to the check URL, and the response content is checked to see if it contains the expected text. If the expected text is not found, an assertion error is raised, indicating that the check failed."
10757,"def potential_from_grid(self, grid):
        """"""
        Calculate the potential at a given set of arc-second gridded coordinates.

        Parameters
        ----------
        grid : grids.RegularGrid
            The grid of (y,x) arc-second coordinates the deflection angles are computed on.
        """"""
        eta = self.grid_to_elliptical_radii(grid)
        return 2.0 * self.einstein_radius_rescaled * eta",python,"Calculate the potential at a given set of arc-second gridded coordinates.

        Parameters
        ----------
        grid : grids.RegularGrid
            The grid of (y,x) arc-second coordinates the deflection angles are computed on.",88,"This code defines a method called `potential_from_grid` that calculates the potential at a given set of arc-second gridded coordinates. The method takes one parameter, `grid`, which represents the grid of (y,x) arc-second coordinates the deflection angles are computed on. The method first converts the grid to elliptical radii using the `grid_to_elliptical_radii` method, then it calculates the potential by multiplying the Einstein radius (rescaled) by 2 and the elliptical radii. The calculated potential is then returned."
5081,"def get_callable_handler_function(src, handler):
    """"""Tranlate a string of the form ""module.function"" into a callable
    function.

    :param str src:
      The path to your Lambda project containing a valid handler file.
    :param str handler:
      A dot delimited string representing the `<module>.<function name>`.
    """"""

    # ""cd"" into `src` directory.
    os.chdir(src)

    module_name, function_name = handler.split('.')
    filename = get_handler_filename(handler)

    path_to_module_file = os.path.join(src, filename)
    module = load_source(module_name, path_to_module_file)
    return getattr(module, function_name)",python,"Tranlate a string of the form ""module.function"" into a callable
    function.

    :param str src:
      The path to your Lambda project containing a valid handler file.
    :param str handler:
      A dot delimited string representing the `<module>.<function name>`.",140,"This Python function, `get_callable_handler_function`, takes two parameters: `src` and `handler`. `src` is the path to a Lambda project containing a valid handler file, and `handler` is a string in the format `<module>.<function name>`. The function changes the current working directory to the `src` directory, splits the `handler` string into `module_name` and `function_name`, and gets the filename of the handler. It then constructs the path to the module file, loads the module from this file, and returns the function specified by `function_name` from this module. The returned function can then be called directly."
1589,"def matchPatterns(patterns, keys):
  """"""
  Returns a subset of the keys that match any of the given patterns

  :param patterns: (list) regular expressions to match
  :param keys: (list) keys to search for matches
  """"""
  results = []
  if patterns:
    for pattern in patterns:
      prog = re.compile(pattern)
      for key in keys:
        if prog.match(key):
          results.append(key)
  else:
    return None

  return results",python,"Returns a subset of the keys that match any of the given patterns

  :param patterns: (list) regular expressions to match
  :param keys: (list) keys to search for matches",102,"This function, `matchPatterns`, takes two parameters: `patterns`, a list of regular expressions, and `keys`, a list of keys to search for matches. It returns a list of keys that match any of the given patterns. If the `patterns` list is empty, it returns `None`. For each pattern, it compiles the regular expression and checks each key to see if it matches the pattern. If a key matches, it is added to the `results` list."
11589,"def modules(self, filters=None):
        """"""
        A generator that produces a sequence of paths to files that look to be
        Python modules (e.g., ``*.py``).

        :param filters:
            the regular expressions to use when finding files in the project.
            If not specified, all files are returned.
        :type filters: list(str)
        """"""

        masks = compile_masks(r'\.py$')
        for file_ in self.files(filters=filters):
            if matches_masks(file_, masks):
                yield file_",python,"A generator that produces a sequence of paths to files that look to be
        Python modules (e.g., ``*.py``).

        :param filters:
            the regular expressions to use when finding files in the project.
            If not specified, all files are returned.
        :type filters: list(str)",109,"This Python function, `modules`, is a generator that yields a sequence of paths to files that are Python modules (i.e., files ending with `.py`). It takes an optional parameter `filters` which is a list of regular expressions used to filter the files in the project. If `filters` is not specified, all files are returned. The function compiles a mask to match files ending with `.py` and then iterates over the files in the project. If a file matches the mask, it is yielded by the generator."
4937,"def __verify_arguments(self):
        """"""!
        @brief Checks algorithm's arguments and if some of them is incorrect then exception is thrown.

        """"""
        if self.__kmax > len(self.__data):
            raise ValueError(""K max value '"" + str(self.__kmax) + ""' is bigger than amount of objects '"" +
                             str(len(self.__data)) + ""' in input data."")

        if self.__kmin <= 1:
            raise ValueError(""K min value '"" + str(self.__kmin) + ""' should be greater than 1 (impossible to provide ""
                             ""silhouette score for only one cluster)."")",python,"!
        @brief Checks algorithm's arguments and if some of them is incorrect then exception is thrown.",131,"This code defines a method named `__verify_arguments` that checks the validity of the algorithm's arguments. If the maximum value of 'k' (`__kmax`) is greater than the length of the data, it raises a ValueError indicating that 'k' is larger than the number of objects in the input data. Similarly, if the minimum value of 'k' (`__kmin`) is less than or equal to 1, it raises a ValueError stating that 'k' should be greater than 1 because it's impossible to provide a silhouette score for only one cluster."
5003,"def calculate_local_order_parameter(self, oscillatory_network, start_iteration = None, stop_iteration = None):
        """"""!
        @brief Calculates local order parameter.
        @details Local order parameter or so-called level of local or partial synchronization is calculated by following expression:
        
        \f[
        r_{c}=\left | \sum_{i=0}^{N} \frac{1}{N_{i}} \sum_{j=0}e^{ \theta_{j} - \theta_{i} } \right |;
        \f]
        
        where N - total amount of oscillators in the network and \f$N_{i}\f$ - amount of neighbors of oscillator with index \f$i\f$.
        
        @param[in] oscillatory_network (sync): Sync oscillatory network whose structure of connections is required for calculation.
        @param[in] start_iteration (uint): The first iteration that is used for calculation, if 'None' then the last iteration is used.
        @param[in] stop_iteration (uint): The last iteration that is used for calculation, if 'None' then 'start_iteration' + 1 is used.
        
        @return (list) List of levels of local (partial) synchronization (local order parameter evolution).
        
        """"""

        (start_iteration, stop_iteration) = self.__get_start_stop_iterations(start_iteration, stop_iteration);
        
        if (self._ccore_sync_dynamic_pointer is not None):
            network_pointer = oscillatory_network._ccore_network_pointer;
            return wrapper.sync_dynamic_calculate_local_order(self._ccore_sync_dynamic_pointer, network_pointer, start_iteration, stop_iteration);
        
        sequence_local_order = [];
        for index in range(start_iteration, stop_iteration):
            sequence_local_order.append(order_estimator.calculate_local_sync_order(self.output[index], oscillatory_network));
        
        return sequence_local_order;",python,"!
        @brief Calculates local order parameter.
        @details Local order parameter or so-called level of local or partial synchronization is calculated by following expression:
        
        \f[
        r_{c}=\left | \sum_{i=0}^{N} \frac{1}{N_{i}} \sum_{j=0}e^{ \theta_{j} - \theta_{i} } \right |;
        \f]
        
        where N - total amount of oscillators in the network and \f$N_{i}\f$ - amount of neighbors of oscillator with index \f$i\f$.
        
        @param[in] oscillatory_network (sync): Sync oscillatory network whose structure of connections is required for calculation.
        @param[in] start_iteration (uint): The first iteration that is used for calculation, if 'None' then the last iteration is used.
        @param[in] stop_iteration (uint): The last iteration that is used for calculation, if 'None' then 'start_iteration' + 1 is used.
        
        @return (list) List of levels of local (partial) synchronization (local order parameter evolution).",385,"This Python function, `calculate_local_order_parameter`, calculates the local order parameter (also known as the level of local or partial synchronization) for a given oscillatory network. The local order parameter is calculated using a specific mathematical expression. 

The function takes three parameters: `oscillatory_network`, `start_iteration`, and `stop_iteration`. The `oscillatory_network` is the network whose structure of connections is required for the calculation. The `start_iteration` and `stop_iteration` parameters define the range of iterations used for the calculation. If these are not provided, default values are used.

The function first determines the start and stop iterations. If the `_ccore_sync_dynamic_pointer` attribute is not None, it uses a wrapper function to calculate the local order parameter. If this attribute is None, it calculates the local order parameter for each iteration in the specified range and appends it to the `sequence_local_order` list.

The function returns a list of local order parameters."
6275,"def find_boundary_types(model, boundary_type, external_compartment=None):
    """"""Find specific boundary reactions.

    Arguments
    ---------
    model : cobra.Model
        A cobra model.
    boundary_type : str
        What boundary type to check for. Must be one of
        ""exchange"", ""demand"", or ""sink"".
    external_compartment : str or None
        The id for the external compartment. If None it will be detected
        automatically.

    Returns
    -------
    list of cobra.reaction
        A list of likely boundary reactions of a user defined type.
    """"""
    if not model.boundary:
        LOGGER.warning(""There are no boundary reactions in this model. ""
                       ""Therefore specific types of boundary reactions such ""
                       ""as 'exchanges', 'demands' or 'sinks' cannot be ""
                       ""identified."")
        return []
    if external_compartment is None:
        external_compartment = find_external_compartment(model)
    return model.reactions.query(
        lambda r: is_boundary_type(r, boundary_type, external_compartment))",python,"Find specific boundary reactions.

    Arguments
    ---------
    model : cobra.Model
        A cobra model.
    boundary_type : str
        What boundary type to check for. Must be one of
        ""exchange"", ""demand"", or ""sink"".
    external_compartment : str or None
        The id for the external compartment. If None it will be detected
        automatically.

    Returns
    -------
    list of cobra.reaction
        A list of likely boundary reactions of a user defined type.",224,"This Python function, `find_boundary_types`, is designed to find specific boundary reactions in a given model. The function takes three parameters: `model`, `boundary_type`, and `external_compartment`. The `model` is a cobra model, `boundary_type` is a string that specifies the type of boundary to check for (must be ""exchange"", ""demand"", or ""sink""), and `external_compartment` is the id for the external compartment, which is detected automatically if not provided. 

The function first checks if the model has any boundary reactions. If not, it logs a warning and returns an empty list. If the `external_compartment` is not provided, it calls the `find_external_compartment` function to determine it. Finally, it returns a list of reactions from the model that match the specified boundary type and external compartment."
20321,"def htmlFor(self,fname):
        """"""return appropriate HTML determined by file extension.""""""
        if os.path.splitext(fname)[1].lower() in ['.jpg','.png']:
            html='<a href=""%s""><img src=""%s""></a>'%(fname,fname)
            if ""_tif_"" in fname:
                html=html.replace('<img ','<img class=""datapic micrograph""')
            if ""_plot_"" in fname:
                html=html.replace('<img ','<img class=""datapic intrinsic"" ')
            if ""_experiment_"" in fname:
                html=html.replace('<img ','<img class=""datapic experiment"" ')
        elif os.path.splitext(fname)[1].lower() in ['.html','.htm']:
            html='LINK: %s'%fname
        else:
            html='<br>Not sure how to show: [%s]</br>'%fname
        return html",python,return appropriate HTML determined by file extension.,181,"This function, `htmlFor`, takes a filename as an argument and returns an HTML string based on the file's extension. If the file is an image (either .jpg or .png), it creates an HTML string to display the image as a clickable link. It also checks for specific substrings in the filename to add a class to the image tag for styling purposes. If the file is an HTML file, it simply creates a text link. If the file extension is neither an image nor an HTML file, it returns a string indicating uncertainty about how to display the file."
7315,"def show_message(self, message_str):
        """"""Show a temporary message.""""""
        if self._message_handle is not None:
            self._message_handle.cancel()
        self._message_handle = asyncio.get_event_loop().call_later(
            self._MESSAGE_DELAY_SECS, self._clear_message
        )
        self._message = message_str
        self._update()",python,Show a temporary message.,75,"This Python method, `show_message`, is part of a class and is used to display a temporary message. If a message is currently being displayed (`self._message_handle is not None`), it cancels that message. It then sets a new message to be displayed after a delay (`self._MESSAGE_DELAY_SECS`). The message to be displayed is passed as an argument (`message_str`) to the method. Finally, it calls the `_update` method, presumably to refresh the display or interface where the message is shown."
13204,"def showstats(parsedict):
    """""" loads assembly or dies, and print stats to screen """"""

    #project_dir = parsedict['1']
    project_dir = parsedict[""project_dir""]
    if not project_dir:
        project_dir = ""./""
    ## Be nice if somebody also puts in the file extension
    #assembly_name = parsedict['0']
    assembly_name = parsedict[""assembly_name""]
    my_assembly = os.path.join(project_dir, assembly_name)

    ## If the project_dir doesn't exist don't even bother trying harder.
    if not os.path.isdir(project_dir):
        msg = """"""
    Trying to print stats for Assembly ({}) that doesn't exist. You must 
    first run steps before you can show results.
    """""".format(project_dir)
        sys.exit(msg)

    if not assembly_name:
        msg = """"""
    Assembly name is not set in params.txt, meaning it was either changed or
    erased since the Assembly was started. Please restore the original name. 
    You can find the name of your Assembly in the ""project dir"": {}.
    """""".format(project_dir)
        raise IPyradError(msg)

    data = ip.load_json(my_assembly, quiet=True, cli=True)

    print(""\nSummary stats of Assembly {}"".format(data.name) \
         +""\n------------------------------------------------"")
    
    if not data.stats.empty:
        print(data.stats)
        print(""\n\nFull stats files""\
         +""\n------------------------------------------------"")

        fullcurdir = os.path.realpath(os.path.curdir)
        for i in range(1, 8):
            #enumerate(sorted(data.stats_files)):
            key = ""s""+str(i)
            try:
                val = data.stats_files[key]
                val = val.replace(fullcurdir, ""."")                
                print(""step {}: {}"".format(i, val))
            except (KeyError, AttributeError):
                print(""step {}: None"".format(i))
        print(""\n"")
    else:
        print(""No stats to display"")",python,"loads assembly or dies, and print stats to screen",411,"This Python function, `showstats()`, takes a dictionary `parsedict` as an argument, which contains the project directory and assembly name. It checks if the project directory exists and if the assembly name is provided. If either of these conditions is not met, it raises an error and exits the program. If the conditions are met, it loads the assembly data from a JSON file and prints the summary statistics of the assembly. If there are no statistics to display, it prints a message saying so. If there are statistics, it prints them and then iterates over a range of 1 to 7, attempting to print the statistics files for each step. If a step doesn't have a statistics file, it prints ""None"" for that step."
21035,"def _print_general_vs_table(self, idset1, idset2):
        """"""
        :param idset1:
        :param idset2:
        """"""
        ref1name = ''
        set1_hasref = isinstance(idset1, idset_with_reference)
        if set1_hasref:
            ref1arr = np.array(idset1.reflst)
            ref1name = idset1.refname

        ref2name = ref1name
        set2_hasref = isinstance(idset2, idset_with_reference)
        if set2_hasref:
            ref2arr = np.array(idset2.reflst)
            ref2name = idset2.refname
        else:
            ref2name = ref1name

        #First show a general table
        hdr11 = '{0} > {1}'.format(idset1.name, idset2.name)
        hdr12 = '{0} > {1} {2}'.format(idset1.name, idset2.name, ref2name)
        hdr13 = '{0} < {1}'.format(idset1.name, idset2.name)
        hdr14 = '{0} < {1} {2}'.format(idset1.name, idset2.name, ref1name)
        table = [[hdr11, hdr12, hdr13, hdr14]]

        set1 = set(idset1)
        set2 = set(idset2)
        row11 = list(set1 - set2)
        if set1_hasref:
            row12 = [ref1arr[np.where(idset1 == nom)][0] for nom in row11]
        else:
            row12 = ['Not found' for _ in row11]

        row13 = list(set2 - set1)
        if set2_hasref:
            row14 = [ref2arr[np.where(idset2 == nom)][0] for nom in row13]
        else:
            row14 = ['Not found' for _ in row13]

        tablst = self._tabulate_4_lists(row11, row12, row13, row14)
        table.extend(tablst)

        if len(table) > 1:
            print(tabulate(table, headers='firstrow'))
            print('\n')",python,":param idset1:
        :param idset2:",471,"This Python function, `_print_general_vs_table`, compares two sets of IDs (`idset1` and `idset2`) and prints a table showing the differences between them. 

First, it checks if each set has a reference (is an instance of `idset_with_reference`). If so, it extracts the reference list and name. 

Then, it creates headers for the table using the names of the sets and the reference names. 

Next, it converts the ID sets to Python `set` objects and calculates the differences between them. If a set has a reference, it uses the reference array to find corresponding values for the differences; otherwise, it fills the row with 'Not found'. 

Finally, it uses the `_tabulate_4_lists` method to create a list of rows for the table, adds these rows to the table, and if the table has more than one row (i.e., there are differences between the sets), it prints the table using the `tabulate` function."
22832,"def get_context(request, model=None):
    """"""
    Extracts ORB context information from the request.

    :param request: <pyramid.request.Request>
    :param model: <orb.Model> || None

    :return: {<str> key: <variant> value} values, <orb.Context>
    """"""
    # convert request parameters to python
    param_values = get_param_values(request, model=model)

    # extract the full orb context if provided
    context = param_values.pop('orb_context', {})
    if isinstance(context, (unicode, str)):
        context = projex.rest.unjsonify(context)

    # otherwise, extract the limit information
    has_limit = 'limit' in context or 'limit' in param_values

    # create the new orb context
    orb_context = orb.Context(**context)

    # build up context information from the request params
    used = set()
    query_context = {}
    for key in orb.Context.Defaults:
        if key in param_values:
            used.add(key)
            query_context[key] = param_values.get(key)

    # generate a simple query object
    schema_values = {}
    if model:
        # extract match dict items
        for key, value in request.matchdict.items():
            if model.schema().column(key, raise_=False):
                schema_values[key] = value

        # extract payload items
        for key, value in param_values.items():
            root_key = key.split('.')[0]
            schema_object = model.schema().column(root_key, raise_=False) or model.schema().collector(root_key)
            if schema_object:
                value = param_values.pop(key)
                if isinstance(schema_object, orb.Collector) and type(value) not in (tuple, list):
                    value = [value]
                schema_values[key] = value

    # generate the base context information
    query_context['scope'] = {
        'request': request
    }

    # include any request specific scoping or information from the request
    # first, look for default ORB context for all requests
    try:
        default_context = request.orb_default_context

    # then, look for scope specific information for all requests
    except AttributeError:
        try:
            query_context['scope'].update(request.orb_scope)
        except AttributeError:
            pass

    # if request specific context defaults exist, then
    # merge them with the rest of the query context
    else:
        if 'scope' in default_context:
            query_context['scope'].update(default_context.pop('scope'))

        # setup defaults based on the request
        for k, v in default_context.items():
            query_context.setdefault(k, v)

    orb_context.update(query_context)
    return schema_values, orb_context",python,"Extracts ORB context information from the request.

    :param request: <pyramid.request.Request>
    :param model: <orb.Model> || None

    :return: {<str> key: <variant> value} values, <orb.Context>",570,"This Python function, `get_context`, is designed to extract context information from a given request and model. It first converts request parameters to Python, then checks if a full orb context is provided. If not, it extracts limit information. It then creates a new orb context and builds up context information from the request parameters. 

If a model is provided, it extracts match dictionary items and payload items. It then generates base context information and includes any request-specific scoping or information from the request. If request-specific context defaults exist, it merges them with the rest of the query context. 

Finally, it updates the orb context with the query context and returns the schema values and orb context. This function is useful for handling and manipulating data in the context of a web request, particularly when using the orb (Object Relational Bridge) framework."
5345,"def cleanup(self, pin=None):
        """"""Clean up GPIO event detection for specific pin, or all pins if none 
        is specified.
        """"""
        if pin is None:
            self.bbio_gpio.cleanup()
        else:
            self.bbio_gpio.cleanup(pin)",python,"Clean up GPIO event detection for specific pin, or all pins if none 
        is specified.",54,"This code defines a method named `cleanup` in a class. The method takes an optional argument `pin`. If no argument is provided, it calls the `cleanup` method of the `bbio_gpio` object, which is assumed to be a member of the same class, without any arguments. This is intended to clean up GPIO event detection for all pins. If a `pin` argument is provided, it calls the same `cleanup` method but with the `pin` argument, which is intended to clean up GPIO event detection for the specified pin."
13875,"def submit_form(self, form):
        """"""Make `Register` object for submitting the registration form.

        Convert form data to legacy fields if `self.form` is `None`.

        :Parameters:
            - `form`: The form to submit. Its type doesn't have to be ""submit""
              (a ""submit"" form will be created here), so it could be the form
              obtained from `get_form` just with the data entered.

        :return: new registration element
        :returntype: `Register`""""""

        result = Register()
        if self.form:
            result.form = form.make_submit()
            return result

        if ""FORM_TYPE"" not in form or ""jabber:iq:register"" not in form[""FORM_TYPE""].values:
            raise ValueError(""FORM_TYPE is not jabber:iq:register"")

        for field in legacy_fields:
            self.__logger.debug(u""submitted field %r"" % (field, ))
            value = getattr(self, field)
            try:
                form_value = form[field].value
            except KeyError:
                if value:
                    raise ValueError(""Required field with no value!"")
                continue
            setattr(result, field, form_value)

        return result",python,"Make `Register` object for submitting the registration form.

        Convert form data to legacy fields if `self.form` is `None`.

        :Parameters:
            - `form`: The form to submit. Its type doesn't have to be ""submit""
              (a ""submit"" form will be created here), so it could be the form
              obtained from `get_form` just with the data entered.

        :return: new registration element
        :returntype: `Register`",250,"This code defines a function `submit_form` that creates a `Register` object for submitting a registration form. If `self.form` is not `None`, it converts the form data to a submit form and assigns it to the `form` attribute of the `Register` object. If `self.form` is `None`, it checks if the form type is ""jabber:iq:register"". If not, it raises a `ValueError`. Then it iterates over `legacy_fields`, logs the field name, and tries to get the value of the field from the form. If the field is not in the form and the field has a value, it raises a `ValueError`. Otherwise, it sets the field value in the `Register` object. The function returns the `Register` object."
22364,"def imgmin(self):
        """"""
        Lowest value of input image.
        """"""
        if not hasattr(self, '_imgmin'):
            imgmin = _np.min(self.images[0])
            for img in self.images:
                imin = _np.min(img)
                if imin > imgmin:
                    imgmin = imin

            self._imgmin = imgmin
        return _np.min(self.image)",python,Lowest value of input image.,86,"This code defines a method called `imgmin` that calculates the minimum value across all images in the `self.images` list. It first checks if the `_imgmin` attribute exists. If it doesn't, it calculates the minimum value of the first image and then iterates over all images to find the overall minimum value. This value is then stored in `self._imgmin` for future use. The method finally returns the minimum value of `self.image`."
10964,"def sqrt_rc_imp(Ns,alpha,M=6):
    """"""
    A truncated square root raised cosine pulse used in digital communications.

    The pulse shaping factor :math:`0 < \\alpha < 1` is required as well as the
    truncation factor M which sets the pulse duration to be :math:`2*M*T_{symbol}`.
     

    Parameters
    ----------
    Ns : number of samples per symbol
    alpha : excess bandwidth factor on (0, 1), e.g., 0.35
    M : equals RC one-sided symbol truncation factor

    Returns
    -------
    b : ndarray containing the pulse shape

    Notes
    -----
    The pulse shape b is typically used as the FIR filter coefficients
    when forming a pulse shaped digital communications waveform. When 
    square root raised cosine (SRC) pulse is used to generate Tx signals and
    at the receiver used as a matched filter (receiver FIR filter), the 
    received signal is now raised cosine shaped, thus having zero
    intersymbol interference and the optimum removal of additive white 
    noise if present at the receiver input.

    Examples
    --------
    Ten samples per symbol and :math:`\\alpha = 0.35`.

    >>> import matplotlib.pyplot as plt
    >>> from numpy import arange
    >>> from sk_dsp_comm.digitalcom import sqrt_rc_imp
    >>> b = sqrt_rc_imp(10,0.35)
    >>> n = arange(-10*6,10*6+1)
    >>> plt.stem(n,b)
    >>> plt.show()
    """"""
    # Design the filter
    n = np.arange(-M*Ns,M*Ns+1)
    b = np.zeros(len(n))
    Ns *= 1.0
    a = alpha
    for i in range(len(n)):
       if abs(1 - 16*a**2*(n[i]/Ns)**2) <= np.finfo(np.float).eps/2:
           b[i] = 1/2.*((1+a)*np.sin((1+a)*np.pi/(4.*a))-(1-a)*np.cos((1-a)*np.pi/(4.*a))+(4*a)/np.pi*np.sin((1-a)*np.pi/(4.*a)))
       else:
           b[i] = 4*a/(np.pi*(1 - 16*a**2*(n[i]/Ns)**2))
           b[i] = b[i]*(np.cos((1+a)*np.pi*n[i]/Ns) + np.sinc((1-a)*n[i]/Ns)*(1-a)*np.pi/(4.*a))
    return b",python,"A truncated square root raised cosine pulse used in digital communications.

    The pulse shaping factor :math:`0 < \\alpha < 1` is required as well as the
    truncation factor M which sets the pulse duration to be :math:`2*M*T_{symbol}`.
     

    Parameters
    ----------
    Ns : number of samples per symbol
    alpha : excess bandwidth factor on (0, 1), e.g., 0.35
    M : equals RC one-sided symbol truncation factor

    Returns
    -------
    b : ndarray containing the pulse shape

    Notes
    -----
    The pulse shape b is typically used as the FIR filter coefficients
    when forming a pulse shaped digital communications waveform. When 
    square root raised cosine (SRC) pulse is used to generate Tx signals and
    at the receiver used as a matched filter (receiver FIR filter), the 
    received signal is now raised cosine shaped, thus having zero
    intersymbol interference and the optimum removal of additive white 
    noise if present at the receiver input.

    Examples
    --------
    Ten samples per symbol and :math:`\\alpha = 0.35`.

    >>> import matplotlib.pyplot as plt
    >>> from numpy import arange
    >>> from sk_dsp_comm.digitalcom import sqrt_rc_imp
    >>> b = sqrt_rc_imp(10,0.35)
    >>> n = arange(-10*6,10*6+1)
    >>> plt.stem(n,b)
    >>> plt.show()",550,"This Python function, `sqrt_rc_imp`, generates a truncated square root raised cosine pulse, which is commonly used in digital communications. The function takes three parameters: `Ns` (number of samples per symbol), `alpha` (excess bandwidth factor, which should be between 0 and 1), and `M` (one-sided symbol truncation factor, defaulting to 6). 

The function first initializes an array `n` with a range from `-M*Ns` to `M*Ns+1` and an array `b` of zeros with the same length as `n`. It then iterates over each element in `n`, calculating the corresponding value in `b` based on the provided formula. 

The resulting array `b` is typically used as the FIR filter coefficients when forming a pulse-shaped digital communications waveform. When this square root raised cosine (SRC) pulse is used to generate transmission signals and at the receiver used as a matched filter, the received signal is now raised cosine shaped, thus having zero intersymbol interference and the optimum removal of additive white noise if present at the receiver input. 

The function returns the array `b`."
3709,"def jt_aggregate(func, is_create=False, has_pk=False):
    """"""Decorator to aggregate unified_jt-related fields.

    Args:
        func: The CURD method to be decorated.
        is_create: Boolean flag showing whether this method is create.
        has_pk: Boolean flag showing whether this method uses pk as argument.

    Returns:
        A function with necessary click-related attributes whose keyworded
        arguments are aggregated.

    Raises:
        exc.UsageError: Either more than one unified jt fields are
            provided, or none is provided when is_create flag is set.
    """"""
    def helper(kwargs, obj):
        """"""The helper function preceding actual function that aggregates
        unified jt fields.
        """"""
        unified_job_template = None
        for item in UNIFIED_JT:
            if kwargs.get(item, None) is not None:
                jt_id = kwargs.pop(item)
                if unified_job_template is None:
                    unified_job_template = (item, jt_id)
                else:
                    raise exc.UsageError(
                        'More than one unified job template fields provided, '
                        'please tighten your criteria.'
                    )
        if unified_job_template is not None:
            kwargs['unified_job_template'] = unified_job_template[1]
            obj.identity = tuple(list(obj.identity) + ['unified_job_template'])
            return '/'.join([UNIFIED_JT[unified_job_template[0]],
                             str(unified_job_template[1]), 'schedules/'])
        elif is_create:
            raise exc.UsageError('You must provide exactly one unified job'
                                 ' template field during creation.')

    def decorator_without_pk(obj, *args, **kwargs):
        old_endpoint = obj.endpoint
        new_endpoint = helper(kwargs, obj)
        if is_create:
            obj.endpoint = new_endpoint
        result = func(obj, *args, **kwargs)
        obj.endpoint = old_endpoint
        return result

    def decorator_with_pk(obj, pk=None, *args, **kwargs):
        old_endpoint = obj.endpoint
        new_endpoint = helper(kwargs, obj)
        if is_create:
            obj.endpoint = new_endpoint
        result = func(obj, pk=pk, *args, **kwargs)
        obj.endpoint = old_endpoint
        return result

    decorator = decorator_with_pk if has_pk else decorator_without_pk
    for item in CLICK_ATTRS:
        setattr(decorator, item, getattr(func, item, []))
    decorator.__doc__ = func.__doc__

    return decorator",python,"Decorator to aggregate unified_jt-related fields.

    Args:
        func: The CURD method to be decorated.
        is_create: Boolean flag showing whether this method is create.
        has_pk: Boolean flag showing whether this method uses pk as argument.

    Returns:
        A function with necessary click-related attributes whose keyworded
        arguments are aggregated.

    Raises:
        exc.UsageError: Either more than one unified jt fields are
            provided, or none is provided when is_create flag is set.",512,"This Python code defines a decorator function `jt_aggregate` that modifies the behavior of a given function `func` related to CRUD (Create, Read, Update, Delete) operations. The decorator is designed to handle unified job template-related fields. 

The decorator function takes three arguments: `func` (the function to be decorated), `is_create` (a boolean flag indicating if the method is a create operation), and `has_pk` (a boolean flag indicating if the method uses a primary key as an argument).

The decorator function defines two inner functions `decorator_without_pk` and `decorator_with_pk` which modify the behavior of the original function based on whether a primary key is used or not. These inner functions use a helper function to aggregate unified job template fields and adjust the endpoint of the object before and after the function call.

The decorator function also handles exceptions, raising a `UsageError` if more than one unified job template fields are provided, or if none is provided when the `is_create` flag is set.

Finally, the decorator function returns the appropriate decorator based on the `has_pk` flag and sets the click-related attributes and documentation of the decorated function."
5212,"def create_from_snapshot(self, *args, **kwargs):
        """"""
        Creates a Block Storage volume

        Note: Every argument and parameter given to this method will be
        assigned to the object.

        Args:
            name: string - a name for the volume
            snapshot_id: string - unique identifier for the volume snapshot
            size_gigabytes: int - size of the Block Storage volume in GiB
            filesystem_type: string, optional - name of the filesystem type the
                volume will be formated with ('ext4' or 'xfs')
            filesystem_label: string, optional - the label to be applied to the
                filesystem, only used in conjunction with filesystem_type

        Optional Args:
            description: string - text field to describe a volume
        """"""
        data = self.get_data('volumes/',
                             type=POST,
                             params={'name': self.name,
                                     'snapshot_id': self.snapshot_id,
                                     'region': self.region,
                                     'size_gigabytes': self.size_gigabytes,
                                     'description': self.description,
                                     'filesystem_type': self.filesystem_type,
                                     'filesystem_label': self.filesystem_label
                                     })

        if data:
            self.id = data['volume']['id']
            self.created_at = data['volume']['created_at']

        return self",python,"Creates a Block Storage volume

        Note: Every argument and parameter given to this method will be
        assigned to the object.

        Args:
            name: string - a name for the volume
            snapshot_id: string - unique identifier for the volume snapshot
            size_gigabytes: int - size of the Block Storage volume in GiB
            filesystem_type: string, optional - name of the filesystem type the
                volume will be formated with ('ext4' or 'xfs')
            filesystem_label: string, optional - the label to be applied to the
                filesystem, only used in conjunction with filesystem_type

        Optional Args:
            description: string - text field to describe a volume",272,"This Python method, `create_from_snapshot`, is part of a class and is used to create a Block Storage volume from a snapshot. It takes several arguments including name, snapshot_id, size_gigabytes, and optionally filesystem_type, filesystem_label, and description. These arguments are used to set the parameters for a POST request to the 'volumes/' endpoint. The response data from this request is then used to set the 'id' and 'created_at' attributes of the object. If the request is successful, the method returns the object itself."
20251,"def _qsub_collate_and_launch(self, output_dir, error_dir, job_names):
        """"""
        The method that actually runs qsub to invoke the python
        process with the necessary commands to trigger the next
        collation step and next block of jobs.
        """"""

        job_name = ""%s_%s_collate_%d"" % (self.batch_name,
                                         self.job_timestamp,
                                         self.collate_count)

        overrides = [(""-e"",error_dir), ('-N',job_name), (""-o"",output_dir),
                     ('-hold_jid',','.join(job_names))]

        resume_cmds =[""import os, pickle, lancet"",
                      (""pickle_path = os.path.join(%r, 'qlauncher.pkl')""
                       % self.root_directory),
                      ""launcher = pickle.load(open(pickle_path,'rb'))"",
                      ""launcher.collate_and_launch()""]

        cmd_args = [self.command.executable,
                    '-c', ';'.join(resume_cmds)]
        popen_args = self._qsub_args(overrides, cmd_args)

        p = subprocess.Popen(popen_args, stdout=subprocess.PIPE)
        (stdout, stderr) = p.communicate()

        self.debug(stdout)
        if p.poll() != 0:
            raise EnvironmentError(""qsub command exit with code: %d"" % p.poll())

        self.collate_count += 1
        self.message(""Invoked qsub for next batch."")
        return job_name",python,"The method that actually runs qsub to invoke the python
        process with the necessary commands to trigger the next
        collation step and next block of jobs.",301,"This Python function, `_qsub_collate_and_launch`, is used to submit a job to a queueing system (qsub) for batch processing. It first constructs a unique job name using the batch name, job timestamp, and collate count. It then sets up a list of overrides for the qsub command, including error and output directories, job name, and any jobs that this job should wait for before starting.

Next, it creates a list of commands to be executed by the Python interpreter. These commands import necessary modules, load a launcher object from a pickle file, and call the `collate_and_launch` method on the launcher object.

The function then constructs the full command to be executed by qsub, and uses the `subprocess.Popen` function to execute this command. It captures any output or errors from the command, and raises an error if the command exits with a non-zero status.

Finally, it increments the collate count, logs a message indicating that the qsub command has been invoked, and returns the job name."
7421,"def _conditional_links(assembled_specs, app_name):
    """""" Given the assembled specs and app_name, this function will return all apps and services specified in
    'conditional_links' if they are specified in 'apps' or 'services' in assembled_specs. That means that
    some other part of the system has declared them as necessary, so they should be linked to this app """"""
    link_to_apps = []
    potential_links = assembled_specs['apps'][app_name]['conditional_links']
    for potential_link in potential_links['apps']:
        if potential_link in assembled_specs['apps']:
            link_to_apps.append(potential_link)
    for potential_link in potential_links['services']:
        if potential_link in assembled_specs['services']:
            link_to_apps.append(potential_link)
    return link_to_apps",python,"Given the assembled specs and app_name, this function will return all apps and services specified in
    'conditional_links' if they are specified in 'apps' or 'services' in assembled_specs. That means that
    some other part of the system has declared them as necessary, so they should be linked to this app",165,"This function, `_conditional_links`, takes two parameters: `assembled_specs` and `app_name`. `assembled_specs` is a dictionary that contains information about apps and services. `app_name` is the name of a specific app. The function checks if there are any 'conditional_links' specified for the given app in the 'assembled_specs'. If there are, it checks if these linked apps or services exist in the 'assembled_specs'. If they do, they are added to the `link_to_apps` list. The function finally returns this list, which contains all the apps and services that are linked to the given app and exist in the 'assembled_specs'."
16321,"def add_record_length_check(self,
                         code=RECORD_LENGTH_CHECK_FAILED,
                         message=MESSAGES[RECORD_LENGTH_CHECK_FAILED],
                         modulus=1):
        """"""
        Add a record length check, i.e., check whether the length of a record is
        consistent with the number of expected fields.

        Arguments
        ---------

        `code` - problem code to report if a record is not valid, defaults to
        `RECORD_LENGTH_CHECK_FAILED`

        `message` - problem message to report if a record is not valid

        `modulus` - apply the check to every nth record, defaults to 1 (check
        every record)

        """"""

        t = code, message, modulus
        self._record_length_checks.append(t)",python,"Add a record length check, i.e., check whether the length of a record is
        consistent with the number of expected fields.

        Arguments
        ---------

        `code` - problem code to report if a record is not valid, defaults to
        `RECORD_LENGTH_CHECK_FAILED`

        `message` - problem message to report if a record is not valid

        `modulus` - apply the check to every nth record, defaults to 1 (check
        every record)",157,"This code defines a method called `add_record_length_check` in a class. This method is used to add a record length check, which verifies if the length of a record is consistent with the number of expected fields. It takes three parameters: `code`, `message`, and `modulus`. The `code` and `message` are used to report if a record is not valid. The `modulus` parameter is used to apply the check to every nth record, defaulting to 1, which means it checks every record. The method then appends these three parameters as a tuple to the list `_record_length_checks`."
16846,"def authenticate(username, password, service='login', encoding='utf-8',
                 resetcred=True):
    """"""Returns True if the given username and password authenticate for the
    given service.  Returns False otherwise.

    ``username``: the username to authenticate

    ``password``: the password in plain text

    ``service``: the PAM service to authenticate against.
                 Defaults to 'login'

    The above parameters can be strings or bytes.  If they are strings,
    they will be encoded using the encoding given by:

    ``encoding``: the encoding to use for the above parameters if they
                  are given as strings.  Defaults to 'utf-8'

    ``resetcred``: Use the pam_setcred() function to
                   reinitialize the credentials.
                   Defaults to 'True'.
    """"""

    if sys.version_info >= (3,):
        if isinstance(username, str):
            username = username.encode(encoding)
        if isinstance(password, str):
            password = password.encode(encoding)
        if isinstance(service, str):
            service = service.encode(encoding)

    @conv_func
    def my_conv(n_messages, messages, p_response, app_data):
        """"""Simple conversation function that responds to any
        prompt where the echo is off with the supplied password""""""
        # Create an array of n_messages response objects
        addr = calloc(n_messages, sizeof(PamResponse))
        p_response[0] = cast(addr, POINTER(PamResponse))
        for i in range(n_messages):
            if messages[i].contents.msg_style == PAM_PROMPT_ECHO_OFF:
                pw_copy = strdup(password)
                p_response.contents[i].resp = cast(pw_copy, c_char_p)
                p_response.contents[i].resp_retcode = 0
        return 0

    handle = PamHandle()
    conv = PamConv(my_conv, 0)
    retval = pam_start(service, username, byref(conv), byref(handle))

    if retval != 0:
        # TODO: This is not an authentication error, something
        # has gone wrong starting up PAM
        return False

    retval = pam_authenticate(handle, 0)
    auth_success = (retval == 0)

    # Re-initialize credentials (for Kerberos users, etc)
    # Don't check return code of pam_setcred(), it shouldn't matter
    # if this fails
    if auth_success and resetcred:
        retval = pam_setcred(handle, PAM_REINITIALIZE_CRED)

    pam_end(handle, retval)

    return auth_success",python,"Returns True if the given username and password authenticate for the
    given service.  Returns False otherwise.

    ``username``: the username to authenticate

    ``password``: the password in plain text

    ``service``: the PAM service to authenticate against.
                 Defaults to 'login'

    The above parameters can be strings or bytes.  If they are strings,
    they will be encoded using the encoding given by:

    ``encoding``: the encoding to use for the above parameters if they
                  are given as strings.  Defaults to 'utf-8'

    ``resetcred``: Use the pam_setcred() function to
                   reinitialize the credentials.
                   Defaults to 'True'.",531,"This Python function, `authenticate`, is used to authenticate a user's credentials (username and password) against a specified service using PAM (Pluggable Authentication Modules). The function takes five parameters: `username`, `password`, `service` (default is 'login'), `encoding` (default is 'utf-8'), and `resetcred` (default is True). 

If the Python version is 3 or above, the function checks if the parameters are strings and if so, encodes them using the specified encoding. 

The function then defines a conversation function, `my_conv`, which responds to any prompt where the echo is off with the supplied password. 

The function then starts the PAM service and checks if it started successfully. If not, it returns False. 

If the PAM service started successfully, it attempts to authenticate the user. If the authentication is successful and `resetcred` is True, it reinitializes the credentials. 

Finally, it ends the PAM service and returns whether the authentication was successful."
18768,"def _handle_BYWEEKDAY(self, rrkwargs, name, value, **kwargs):
        """"""
        Two ways to specify this: +1MO or MO(+1)
        """"""
        l = []
        for wday in value.split(','):
            if '(' in wday:
                # If it's of the form TH(+1), etc.
                splt = wday.split('(')
                w = splt[0]
                n = int(splt[1][:-1])
            elif len(wday):
                # If it's of the form +1MO
                for i in range(len(wday)):
                    if wday[i] not in '+-0123456789':
                        break
                n = wday[:i] or None
                w = wday[i:]
                if n:
                    n = int(n)
            else:
                raise ValueError(""Invalid (empty) BYDAY specification."")

            l.append(weekdays[self._weekday_map[w]](n))
        rrkwargs[""byweekday""] = l",python,Two ways to specify this: +1MO or MO(+1),213,"This function, `_handle_BYWEEKDAY`, is designed to parse and handle the `BYWEEKDAY` parameter in a recurrence rule. It accepts two formats: `+1MO` or `MO(+1)`, where `MO` represents Monday and `+1` represents the first Monday of the month. The function splits the input value by comma, then checks each weekday. If the weekday contains a parenthesis, it splits the string again and assigns the weekday and the number to `w` and `n` respectively. If the weekday doesn't contain a parenthesis, it iterates over the string until it finds a character that is not a number or a plus/minus sign, then assigns the number and the weekday to `n` and `w` respectively. If the weekday is empty, it raises a ValueError. Finally, it appends the weekday (converted to a weekday object using a mapping dictionary) to a list `l`, and assigns this list to the `byweekday` key in the `rrkwargs` dictionary."
14887,"def datetime_round(dt, period, start=None):
	""""""
	Find the nearest even period for the specified date/time.

	>>> datetime_round(datetime.datetime(2004, 11, 13, 8, 11, 13),
	...     datetime.timedelta(hours = 1))
	datetime.datetime(2004, 11, 13, 8, 0)
	>>> datetime_round(datetime.datetime(2004, 11, 13, 8, 31, 13),
	...     datetime.timedelta(hours = 1))
	datetime.datetime(2004, 11, 13, 9, 0)
	>>> datetime_round(datetime.datetime(2004, 11, 13, 8, 30),
	...     datetime.timedelta(hours = 1))
	datetime.datetime(2004, 11, 13, 9, 0)
	""""""
	result = datetime_mod(dt, period, start)
	if abs(dt - result) >= period // 2:
		result += period
	return result",python,"Find the nearest even period for the specified date/time.

	>>> datetime_round(datetime.datetime(2004, 11, 13, 8, 11, 13),
	...     datetime.timedelta(hours = 1))
	datetime.datetime(2004, 11, 13, 8, 0)
	>>> datetime_round(datetime.datetime(2004, 11, 13, 8, 31, 13),
	...     datetime.timedelta(hours = 1))
	datetime.datetime(2004, 11, 13, 9, 0)
	>>> datetime_round(datetime.datetime(2004, 11, 13, 8, 30),
	...     datetime.timedelta(hours = 1))
	datetime.datetime(2004, 11, 13, 9, 0)",214,"This Python function, `datetime_round`, takes three parameters: a datetime object `dt`, a timedelta object `period`, and an optional datetime object `start`. The function rounds the given datetime `dt` to the nearest period (which is a duration specified as a timedelta). If the difference between the datetime `dt` and the rounded result is greater than or equal to half of the period, the function adds the period to the result. This function is useful for rounding datetime objects to the nearest hour, minute, second, etc., depending on the period provided."
22767,"def fso_unlink(self, path):
    'overlays os.unlink()'
    path = self.deref(path, to_parent=True)
    if not self._lexists(path):
      raise OSError(2, 'No such file or directory', path)
    self._addentry(OverlayEntry(self, path, None))",python,overlays os.unlink(),67,"This code defines a function `fso_unlink` that is designed to delete a file at a given path. The function first dereferences the path, then checks if the file exists at the path. If the file does not exist, it raises an `OSError`. If the file does exist, it adds an entry to a list or similar data structure, indicating that the file at the path should be deleted. The function is intended to overlay, or replace, the `os.unlink()` function, which deletes a file in a real file system."
14534,"def _has_branch(branch):
    """"""Return True if the target branch exists.""""""
    ret = temple.utils.shell('git rev-parse --verify {}'.format(branch),
                             stderr=subprocess.DEVNULL,
                             stdout=subprocess.DEVNULL,
                             check=False)
    return ret.returncode == 0",python,Return True if the target branch exists.,58,"This Python function, `_has_branch(branch)`, checks if a specific Git branch exists. It uses the `git rev-parse --verify` command to verify the existence of the branch. The command is executed in a shell using the `temple.utils.shell` function. The standard output and standard error are redirected to `DEVNULL` to suppress them. If the command executes successfully (return code is 0), the function returns `True`, indicating that the branch exists. If the command fails (return code is not 0), it returns `False`, indicating that the branch does not exist."
14813,"def delete_key(key_name, stash, passphrase, backend):
    """"""Delete a key from the stash

    `KEY_NAME` is the name of the key to delete
    You can provide that multiple times to delete multiple keys at once
    """"""
    stash = _get_stash(backend, stash, passphrase)

    for key in key_name:
        try:
            click.echo('Deleting key {0}...'.format(key))
            stash.delete(key_name=key)
        except GhostError as ex:
            sys.exit(ex)
    click.echo('Keys deleted successfully')",python,"Delete a key from the stash

    `KEY_NAME` is the name of the key to delete
    You can provide that multiple times to delete multiple keys at once",116,"This Python function, `delete_key`, is designed to delete one or more keys from a given stash. The function takes four parameters: `key_name` (the name of the key(s) to be deleted), `stash` (the storage where the keys are kept), `passphrase` (the password for accessing the stash), and `backend` (the system used for storing the keys). 

The function first retrieves the stash using the `_get_stash` function. Then, it iterates over each key in `key_name` and attempts to delete it from the stash. If the key deletion is successful, it prints a message indicating the key has been deleted. If an error occurs during the deletion (a `GhostError`), the function will terminate and print the error message. If all keys are deleted successfully, it prints a success message."
5124,"def make_sentence (sent_text):
    """"""
    construct a sentence text, with proper spacing
    """"""
    lex = []
    idx = 0

    for word in sent_text:
        if len(word) > 0:
            if (idx > 0) and not (word[0] in "",.:;!?-\""'""):
                lex.append("" "")

            lex.append(word)

        idx += 1

    return """".join(lex)",python,"construct a sentence text, with proper spacing",94,"This Python function, `make_sentence`, takes a list of words (`sent_text`) as input and constructs a sentence from it. It iterates over each word in the list, and if the word is not empty, it checks if it's not the first word and doesn't start with a punctuation mark. If both conditions are met, it adds a space before the word. The word is then added to the `lex` list. After iterating over all words, it joins all elements in the `lex` list into a single string without any separator and returns it. This function ensures proper spacing between words in a sentence, except for words following punctuation marks."
10779,"def einstein_radius_in_units(self, unit_length='arcsec', kpc_per_arcsec=None):
        """"""The Einstein Radius of this galaxy, which is the sum of Einstein Radii of its mass profiles.

        If the galaxy is composed of multiple ellipitcal profiles with different axis-ratios, this Einstein Radius \
        may be inaccurate. This is because the differently oriented ellipses of each mass profile """"""

        if self.has_mass_profile:
            return sum(map(lambda p: p.einstein_radius_in_units(unit_length=unit_length, kpc_per_arcsec=kpc_per_arcsec),
                           self.mass_profiles))
        else:
            return None",python,"The Einstein Radius of this galaxy, which is the sum of Einstein Radii of its mass profiles.

        If the galaxy is composed of multiple ellipitcal profiles with different axis-ratios, this Einstein Radius \
        may be inaccurate. This is because the differently oriented ellipses of each mass profile",134,"This code defines a method `einstein_radius_in_units` in a class, presumably for a galaxy object. The method calculates the Einstein Radius of the galaxy, which is the sum of Einstein Radii of its mass profiles. The Einstein Radius is returned in the units specified by the `unit_length` parameter. If the galaxy object does not have a mass profile, the method returns `None`. The method also takes into account the conversion factor between kiloparsecs and arcseconds if provided. However, the method may return inaccurate results if the galaxy is composed of multiple elliptical profiles with different axis-ratios."
19867,"def mkdir_if_not(filepath, ispath=False):
    """"""
    path   mkdir  .
    :param filepath:  
    :return: filpath  
    """"""
    if not ispath:
        p, _ = os.path.split(filepath)
    else:
        p = filepath

    if not p:
        return filepath
    if not os.path.exists(p):
        # M.info('%s not exist, trying mkdir ', p)
        try:
            os.makedirs(p)
        except FileExistsError as e:
            logg.warn(str(e))

    return filepath",python,"path   mkdir  .
    :param filepath:  
    :return: filpath  ",132,"This function, `mkdir_if_not`, checks if a directory exists at the given filepath. If the directory does not exist, it creates one. The `ispath` parameter is used to determine whether the input is a filepath or a directory path. If `ispath` is False, it splits the filepath to get the directory path. If the directory path is not empty and does not exist, it attempts to create the directory. If the directory already exists (throws a `FileExistsError`), it logs a warning. The function returns the original filepath."
6232,"def load_yaml_model(filename):
    """"""
    Load a cobra model from a file in YAML format.

    Parameters
    ----------
    filename : str or file-like
        File path or descriptor that contains the YAML document describing the
        cobra model.

    Returns
    -------
    cobra.Model
        The cobra model as represented in the YAML document.

    See Also
    --------
    from_yaml : Load from a string.
    """"""
    if isinstance(filename, string_types):
        with io.open(filename, ""r"") as file_handle:
            return model_from_dict(yaml.load(file_handle))
    else:
        return model_from_dict(yaml.load(filename))",python,"Load a cobra model from a file in YAML format.

    Parameters
    ----------
    filename : str or file-like
        File path or descriptor that contains the YAML document describing the
        cobra model.

    Returns
    -------
    cobra.Model
        The cobra model as represented in the YAML document.

    See Also
    --------
    from_yaml : Load from a string.",136,"This Python function, `load_yaml_model`, is designed to load a cobra model from a file in YAML format. The function takes one parameter, `filename`, which can be either a string representing the file path or a file-like object. If `filename` is a string, the function opens the file in read mode and loads the YAML document using the `yaml.load` function. The loaded YAML is then converted into a cobra model using the `model_from_dict` function. If `filename` is not a string, it is assumed to be a file-like object and is directly passed to `yaml.load` and `model_from_dict`. The function returns the loaded cobra model."
20421,"def traverse_local_prefs(stepback=0):
    """"""Generator to walk through variables considered as preferences
    in locals dict of a given frame.

    :param int stepback:

    :rtype: tuple

    """"""
    locals_dict = get_frame_locals(stepback+1)
    for k in locals_dict:
        if not k.startswith('_') and k.upper() == k:
            yield k, locals_dict",python,"Generator to walk through variables considered as preferences
    in locals dict of a given frame.

    :param int stepback:

    :rtype: tuple",83,"This Python function, `traverse_local_prefs`, is a generator that traverses through local variables in a given frame that are considered as preferences. The function takes an optional parameter `stepback` which defaults to 0. It uses the function `get_frame_locals` to get the local variables from the frame that is `stepback+1` steps back from the current frame. It then iterates over these local variables. If a variable does not start with an underscore and its name is in uppercase, the function yields a tuple containing the variable's name and the dictionary of local variables."
822,"def _inhibitColumnsGlobal(self, overlaps, density):
    """"""
    Perform global inhibition. Performing global inhibition entails picking the
    top 'numActive' columns with the highest overlap score in the entire
    region. At most half of the columns in a local neighborhood are allowed to
    be active. Columns with an overlap score below the 'stimulusThreshold' are
    always inhibited.

    :param overlaps: an array containing the overlap score for each  column.
                    The overlap score for a column is defined as the number
                    of synapses in a ""connected state"" (connected synapses)
                    that are connected to input bits which are turned on.
    :param density: The fraction of columns to survive inhibition.
    @return list with indices of the winning columns
    """"""
    #calculate num active per inhibition area
    numActive = int(density * self._numColumns)

    # Calculate winners using stable sort algorithm (mergesort)
    # for compatibility with C++
    sortedWinnerIndices = numpy.argsort(overlaps, kind='mergesort')

    # Enforce the stimulus threshold
    start = len(sortedWinnerIndices) - numActive
    while start < len(sortedWinnerIndices):
      i = sortedWinnerIndices[start]
      if overlaps[i] >= self._stimulusThreshold:
        break
      else:
        start += 1

    return sortedWinnerIndices[start:][::-1]",python,"Perform global inhibition. Performing global inhibition entails picking the
    top 'numActive' columns with the highest overlap score in the entire
    region. At most half of the columns in a local neighborhood are allowed to
    be active. Columns with an overlap score below the 'stimulusThreshold' are
    always inhibited.

    :param overlaps: an array containing the overlap score for each  column.
                    The overlap score for a column is defined as the number
                    of synapses in a ""connected state"" (connected synapses)
                    that are connected to input bits which are turned on.
    :param density: The fraction of columns to survive inhibition.
    @return list with indices of the winning columns",294,"This Python function, `_inhibitColumnsGlobal`, performs global inhibition on a set of columns based on their overlap scores. Global inhibition involves selecting the top 'numActive' columns with the highest overlap scores in the entire region. The overlap score for a column is defined as the number of synapses in a ""connected state"" that are connected to input bits which are turned on. 

The function takes two parameters: `overlaps`, an array containing the overlap score for each column, and `density`, the fraction of columns to survive inhibition. 

The function first calculates the number of active columns per inhibition area. Then, it sorts the overlap scores in ascending order using a stable sort algorithm (mergesort). 

Next, it enforces a stimulus threshold. It iterates through the sorted overlap scores from the end (highest scores) towards the beginning. If it encounters a score that is equal to or greater than the stimulus threshold, it breaks the loop. 

Finally, it returns the indices of the winning columns in descending order of their overlap scores. The winning columns are those with overlap scores above the stimulus threshold and are among the top 'numActive' columns."
15018,"def get_network(context, id, fields=None):
    """"""Retrieve a network.

    : param context: neutron api request context
    : param id: UUID representing the network to fetch.
    : param fields: a list of strings that are valid keys in a
        network dictionary as listed in the RESOURCE_ATTRIBUTE_MAP
        object in neutron/api/v2/attributes.py. Only these fields
        will be returned.
    """"""
    LOG.info(""get_network %s for tenant %s fields %s"" %
             (id, context.tenant_id, fields))

    network = db_api.network_find(context=context, limit=None, sorts=['id'],
                                  marker=None, page_reverse=False,
                                  id=id, join_subnets=True, scope=db_api.ONE)
    if not network:
        raise n_exc.NetworkNotFound(net_id=id)
    return v._make_network_dict(network, fields=fields)",python,"Retrieve a network.

    : param context: neutron api request context
    : param id: UUID representing the network to fetch.
    : param fields: a list of strings that are valid keys in a
        network dictionary as listed in the RESOURCE_ATTRIBUTE_MAP
        object in neutron/api/v2/attributes.py. Only these fields
        will be returned.",182,"This Python function, `get_network`, is used to retrieve a network based on its unique identifier (UUID). It takes three parameters: `context` which is the request context of the Neutron API, `id` which is the UUID of the network to be fetched, and `fields` which is a list of strings that are valid keys in a network dictionary. 

The function logs the information about the network retrieval, then it uses the `network_find` method from the `db_api` module to find the network in the database. If the network is not found, it raises a `NetworkNotFound` exception. If the network is found, it uses the `_make_network_dict` method to create a dictionary of the network data, including only the fields specified in the `fields` parameter."
14998,"def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""
    engine = create_engine(
        neutron_config.database.connection,
        poolclass=pool.NullPool)

    connection = engine.connect()
    context.configure(
        connection=connection,
        target_metadata=target_metadata)

    try:
        with context.begin_transaction():
            context.run_migrations()
    finally:
        connection.close()",python,"Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.",101,"This code defines a function `run_migrations_online()` that runs database migrations in an 'online' mode. It first creates a database engine using the connection details from the `neutron_config` configuration and a `NullPool` for connection pooling. It then establishes a connection to the database and configures the migration context with this connection and a target metadata. The migrations are run within a transaction block to ensure atomicity. If any migration fails, all changes made in this transaction will be rolled back. Finally, regardless of whether the migrations were successful or not, the database connection is closed."
16824,"def set_primary(self):
        """"""
        Set this email address as the user's primary email.
        """"""
        query = EmailAddress.objects.filter(is_primary=True, user=self.user)
        query = query.exclude(pk=self.pk)

        # The transaction is atomic so there is never a gap where a user
        # has no primary email address.
        with transaction.atomic():
            query.update(is_primary=False)

            self.is_primary = True
            self.save()

        logger.info(
            ""Set %s as the primary email address for %s."",
            self.email,
            self.user,
        )",python,Set this email address as the user's primary email.,120,"This Python code defines a method called `set_primary` that sets the current email address as the primary email for a user. It first queries for any existing primary email addresses for the user and excludes the current email address from the query. Then, within an atomic transaction, it sets the `is_primary` field of all other email addresses to `False` and sets the `is_primary` field of the current email address to `True`. This ensures that there is no point in time where a user doesn't have a primary email address. Finally, it logs a message indicating that the primary email address for the user has been updated."
2524,"def get(self, cluster, environ, topology, container):
    '''
    :param cluster:
    :param environ:
    :param topology:
    :param container:
    :return:
    '''
    offset = self.get_argument(""offset"")
    length = self.get_argument(""length"")
    path = self.get_argument(""path"")

    data = yield access.get_container_file_data(cluster, environ, topology, container, path,
                                                offset, length)

    self.write(data)
    self.finish()",python,":param cluster:
    :param environ:
    :param topology:
    :param container:
    :return:",100,"This code defines a method called `get` in a class (likely a web request handler). The method takes four parameters: `cluster`, `environ`, `topology`, and `container`. It then retrieves three arguments: `offset`, `length`, and `path` from the request. These arguments are used to fetch specific data from a container file in a certain environment, cluster, and topology using the `access.get_container_file_data` method. The fetched data is then written to the response and the response is finished, ready to be sent back to the client."
15464,"def replies(self):
        """""" returns a list of strings """"""
        fs_reply_path = join(self.fs_replies_path, 'message_001.txt')
        if exists(fs_reply_path):
            return [load(open(fs_reply_path, 'r'))]
        else:
            return []",python,returns a list of strings,56,"This code defines a method named `replies` that returns a list of strings. It constructs a file path (`fs_reply_path`) by joining `self.fs_replies_path` and 'message_001.txt'. If a file exists at this path, it opens the file in read mode, loads its content, and returns it as a single-item list. If the file does not exist, it returns an empty list."
8503,"def verify(self, **kwargs):
        """""" Verify a message with an account's memo key

            :param str account: (optional) the account that owns the bet
                (defaults to ``default_account``)

            :returns: True if the message is verified successfully
            :raises InvalidMessageSignature if the signature is not ok
        """"""
        if not isinstance(self.message, dict):
            try:
                self.message = json.loads(self.message)
            except Exception:
                raise ValueError(""Message must be valid JSON"")

        payload = self.message.get(""payload"")
        assert payload, ""Missing payload""
        payload_dict = {k[0]: k[1] for k in zip(payload[::2], payload[1::2])}
        signature = self.message.get(""signature"")

        account_name = payload_dict.get(""from"").strip()
        memo_key = payload_dict.get(""key"").strip()

        assert account_name, ""Missing account name 'from'""
        assert memo_key, ""missing 'key'""

        try:
            self.publickey_class(memo_key, prefix=self.blockchain.prefix)
        except Exception:
            raise InvalidMemoKeyException(""The memo key in the message is invalid"")

        # Load account from blockchain
        try:
            account = self.account_class(
                account_name, blockchain_instance=self.blockchain
            )
        except AccountDoesNotExistsException:
            raise AccountDoesNotExistsException(
                ""Could not find account {}. Are you connected to the right chain?"".format(
                    account_name
                )
            )

        # Test if memo key is the same as on the blockchain
        if not account[""options""][""memo_key""] == memo_key:
            raise WrongMemoKey(
                ""Memo Key of account {} on the Blockchain "".format(account[""name""])
                + ""differs from memo key in the message: {} != {}"".format(
                    account[""options""][""memo_key""], memo_key
                )
            )

        # Ensure payload and signed match
        signed_target = json.dumps(self.message.get(""payload""), separators=("","", "":""))
        signed_actual = self.message.get(""signed"")
        assert (
            signed_target == signed_actual
        ), ""payload doesn't match signed message: \n{}\n{}"".format(
            signed_target, signed_actual
        )

        # Reformat message
        enc_message = self.message.get(""signed"")

        # Verify Signature
        pubkey = verify_message(enc_message, unhexlify(signature))

        # Verify pubky
        pk = self.publickey_class(
            hexlify(pubkey).decode(""ascii""), prefix=self.blockchain.prefix
        )
        if format(pk, self.blockchain.prefix) != memo_key:
            raise InvalidMessageSignature(""The signature doesn't match the memo key"")

        self.signed_by_account = account
        self.signed_by_name = account[""name""]
        self.plain_message = payload_dict.get(""text"")

        return True",python,"Verify a message with an account's memo key

            :param str account: (optional) the account that owns the bet
                (defaults to ``default_account``)

            :returns: True if the message is verified successfully
            :raises InvalidMessageSignature if the signature is not ok",600,"This Python function, `verify`, is used to verify a message with an account's memo key. It first checks if the message is a valid JSON, then extracts the payload and signature from the message. It also retrieves the account name and memo key from the payload. 

The function then checks if the memo key is valid and if the account exists on the blockchain. It also verifies if the memo key in the message matches the one on the blockchain. 

Next, it ensures that the payload and the signed message match. It then verifies the signature of the message and checks if the public key matches the memo key. 

If all these checks pass, it sets the account as the signer of the message, stores the account name and the plain message text, and returns True. If any of these checks fail, it raises an appropriate exception."
11596,"def get_tools():
    """"""
    Retrieves the TidyPy tools that are available in the current Python
    environment.

    The returned dictionary has keys that are the tool names and values are the
    tool classes.

    :rtype: dict
    """"""

    # pylint: disable=protected-access

    if not hasattr(get_tools, '_CACHE'):
        get_tools._CACHE = dict()
        for entry in pkg_resources.iter_entry_points('tidypy.tools'):
            try:
                get_tools._CACHE[entry.name] = entry.load()
            except ImportError as exc:  # pragma: no cover
                output_error(
                    'Could not load tool ""%s"" defined by ""%s"": %s' % (
                        entry,
                        entry.dist,
                        exc,
                    ),
                )
    return get_tools._CACHE",python,"Retrieves the TidyPy tools that are available in the current Python
    environment.

    The returned dictionary has keys that are the tool names and values are the
    tool classes.

    :rtype: dict",165,"This Python function, `get_tools()`, retrieves the TidyPy tools available in the current Python environment. It uses a cache (a dictionary stored as a function attribute) to store and quickly retrieve the tools. If the cache is not yet initialized, it iterates over the TidyPy tools entry points, attempting to load each one. If a tool fails to load due to an ImportError, it outputs an error message. The function returns the cache of tools."
6606,"def tradepileDelete(self, trade_id):  # item_id instead of trade_id?
        """"""Remove card from tradepile.

        :params trade_id: Trade id.
        """"""
        method = 'DELETE'
        url = 'trade/%s' % trade_id

        self.__request__(method, url)  # returns nothing
        # TODO: validate status code
        return True",python,"Remove card from tradepile.

        :params trade_id: Trade id.",82,"This code defines a method called `tradepileDelete` in Python, which is used to remove a card from a trade pile. The method takes a `trade_id` as a parameter, which is used to identify the specific card to be removed. The method sends a DELETE request to the URL 'trade/{trade_id}', where {trade_id} is replaced by the actual ID of the trade. The `__request__` method is used to send the request. The method currently does not validate the status code of the response, which is a task marked as TODO. The method returns True after sending the request, regardless of the outcome."
11927,"def render_to_string(template, extra=None):
    """"""
    Renders the given template to a string.
    """"""
    from jinja2 import Template
    extra = extra or {}
    final_fqfn = find_template(template)
    assert final_fqfn, 'Template not found: %s' % template
    template_content = open(final_fqfn, 'r').read()
    t = Template(template_content)
    if extra:
        context = env.copy()
        context.update(extra)
    else:
        context = env
    rendered_content = t.render(**context)
    rendered_content = rendered_content.replace('&quot;', '""')
    return rendered_content",python,Renders the given template to a string.,134,"This Python function, `render_to_string`, takes a template file and an optional dictionary of extra variables as input. It uses the Jinja2 library to render the template into a string. The function first finds the template file and reads its content. If the template file is not found, it raises an assertion error. Then, it creates a new Jinja2 Template object with the template content. If there are extra variables provided, it updates the context (a dictionary of variables that will be used in the template) with these extra variables. Finally, it renders the template with the context, replaces any '&quot;' with double quotes in the rendered content, and returns the rendered content as a string."
7794,"def get_preferred_collection(self):
        """"""If self.preferred_collection contains a D-Bus path,
        the collection at that address is returned. Otherwise,
        the default collection is returned.
        """"""
        bus = secretstorage.dbus_init()
        try:
            if hasattr(self, 'preferred_collection'):
                collection = secretstorage.Collection(
                    bus, self.preferred_collection)
            else:
                collection = secretstorage.get_default_collection(bus)
        except exceptions.SecretStorageException as e:
            raise InitError(""Failed to create the collection: %s."" % e)
        if collection.is_locked():
            collection.unlock()
            if collection.is_locked():  # User dismissed the prompt
                raise KeyringLocked(""Failed to unlock the collection!"")
        return collection",python,"If self.preferred_collection contains a D-Bus path,
        the collection at that address is returned. Otherwise,
        the default collection is returned.",154,"This Python function, `get_preferred_collection`, is part of a class and is used to retrieve a specific collection of secrets (like passwords) from a D-Bus path. If the object has a `preferred_collection` attribute, it will try to get the collection at that D-Bus path. If it doesn't have this attribute, it will get the default collection. If there's an error during this process, it raises an `InitError`. If the collection is locked, it tries to unlock it. If it's still locked after that (meaning the user dismissed the unlock prompt), it raises a `KeyringLocked` error. Finally, it returns the collection."
14719,"def list_shares(self, group_id, depth=1):
        """"""
        Retrieves a list of all shares though a group.

        :param      group_id: The unique ID of the group.
        :type       group_id: ``str``

        :param      depth: The depth of the response data.
        :type       depth: ``int``

        """"""
        response = self._perform_request(
            '/um/groups/%s/shares?depth=%s' % (group_id, str(depth)))

        return response",python,"Retrieves a list of all shares though a group.

        :param      group_id: The unique ID of the group.
        :type       group_id: ``str``

        :param      depth: The depth of the response data.
        :type       depth: ``int``",108,"This code defines a function `list_shares` that retrieves a list of all shares through a group. The function takes two parameters: `group_id` which is a unique identifier for a group, and `depth` which determines the depth of the response data. The function makes a request to a specific URL (which includes the group_id and depth as parameters) using the `_perform_request` method, and then returns the response from this request."
4145,"def calc_list_average(l):
    """"""
    Calculates the average value of a list of numbers
    Returns a float
    """"""
    total = 0.0
    for value in l:
        total += value
    return total / len(l)",python,"Calculates the average value of a list of numbers
    Returns a float",52,"This code defines a function called `calc_list_average` that calculates the average of a list of numbers. It initializes a variable `total` to 0.0, then iterates over each number in the list, adding it to `total`. Finally, it returns the total divided by the length of the list, which gives the average."
2887,"def get_heron_config():
  '''
  Get config opts from the global variable
  :return:
  '''
  opt_list = []
  for (key, value) in config_opts.items():
    opt_list.append('%s=%s' % (key, value))

  all_opts = (','.join(opt_list)).replace(' ', '%%%%')
  return all_opts",python,"Get config opts from the global variable
  :return:",75,"This Python function, `get_heron_config()`, retrieves configuration options from a global variable `config_opts`, which is expected to be a dictionary. It iterates over each key-value pair in the dictionary, formats them as a string in the format 'key=value', and appends each string to a list `opt_list`. After all options have been processed, it joins all the strings in the list into a single string, separated by commas. It then replaces any spaces in the string with '%%%%'. The resulting string is returned by the function."
13025,"def _step7func(self, samples, force, ipyclient):
        """""" Step 7: Filter and write output files """"""

        ## Get sample objects from list of strings
        samples = _get_samples(self, samples)

        if self._headers:
            print(""\n  Step 7: Filter and write output files for {} Samples"".\
                  format(len(samples)))

        ## Check if all/none of the samples are in the self.database
        try:
            with h5py.File(self.clust_database, 'r') as io5:
                dbset = set(io5[""seqs""].attrs['samples'])
                iset = set([i.name for i in samples])

                ## TODO: Handle the case where dbdiff is not empty?
                ## This might arise if someone tries to branch and remove
                ## samples at step 7.
                dbdiff = dbset.difference(iset)
                idiff = iset.difference(dbset)
                if idiff:
                    print(NOT_CLUSTERED_YET\
                    .format(self.database, "", "".join(list(idiff))))

                    ## The the old way that failed unless all samples were
                    ## clustered successfully in step 6. Adding some flexibility
                    ## to allow writing output even if some samples failed.
                    ## raise IPyradWarningExit(msg)

                    ## Remove the samples that aren't ready for writing out
                    ## i.e. only proceed with the samples that are actually
                    ## present in the db
                    samples = [x for x in samples if x.name not in idiff]
        except (IOError, ValueError):
            raise IPyradError(FIRST_RUN_6.format(self.clust_database))

        if not force:
            outdir = os.path.join(self.dirs.project, self.name+""_outfiles"")
            if os.path.exists(outdir):
                raise IPyradWarningExit(OUTPUT_EXISTS.format(outdir))

        ## Run step7
        assemble.write_outfiles.run(self, samples, force, ipyclient)",python,Step 7: Filter and write output files,417,"This Python function, `_step7func`, is part of a larger program and is responsible for filtering and writing output files for a set of samples. It first retrieves sample objects from a list of strings. It then checks if all or none of the samples are in the database. If there are samples that are not in the database, it prints a warning and removes those samples from the list. If there is an IOError or ValueError when trying to access the database, it raises an error. If the `force` parameter is not set, it checks if the output directory already exists and raises a warning if it does. Finally, it calls the `run` function from the `assemble.write_outfiles` module to write the output files."
15727,"def spin(self, color, degree, *args):
        """""" Spin color by degree. (Increase / decrease hue)
        args:
            color (str): color
            degree (str): percentage
        raises:
            ValueError
        returns:
            str
        """"""
        if color and degree:
            if isinstance(degree, string_types):
                degree = float(degree.strip('%'))
            h, l, s = self._hextohls(color)
            h = ((h * 360.0) + degree) % 360.0
            h = 360.0 + h if h < 0 else h
            rgb = colorsys.hls_to_rgb(h / 360.0, l, s)
            color = (utility.convergent_round(c * 255) for c in rgb)
            return self._rgbatohex(color)
        raise ValueError('Illegal color values')",python,"Spin color by degree. (Increase / decrease hue)
        args:
            color (str): color
            degree (str): percentage
        raises:
            ValueError
        returns:
            str",188,"This Python function, `spin`, adjusts the hue of a given color by a specified degree. It takes in two mandatory arguments: `color` and `degree`, and an optional argument `*args`. If `degree` is a string, it converts it to a float. The function then converts the color from hexadecimal to HLS (Hue, Lightness, Saturation), adjusts the hue by the given degree, and converts it back to RGB. The RGB values are then rounded and converted back to hexadecimal. If the color or degree is not provided, or if the color cannot be converted, the function raises a `ValueError`."
19407,"def fit(self, X, y=None):
        '''
        Specify the data to which kernel values should be computed.

        Parameters
        ----------
        X : list of arrays or :class:`skl_groups.features.Features`
            The bags to compute ""to"".
        '''
        self.features_ = as_features(X, stack=True, bare=True)
        # TODO: could precompute things like squared norms if kernel == ""rbf"".
        # Probably should add support to sklearn instead of hacking it here.
        return self",python,"Specify the data to which kernel values should be computed.

        Parameters
        ----------
        X : list of arrays or :class:`skl_groups.features.Features`
            The bags to compute ""to"".",104,"This is a method named 'fit' in a Python class. The method takes two parameters: 'X' and 'y'. 'X' is a list of arrays or an instance of the 'Features' class from the 'skl_groups.features' module. The 'y' parameter is optional. The purpose of this method is to specify the data to which kernel values should be computed. The method converts 'X' into features using the 'as_features' function with 'stack' and 'bare' set to True. The result is stored in the 'features_' attribute of the class. The method then returns the instance of the class itself, allowing for method chaining. There's a note about a potential enhancement to precompute certain values if the kernel is ""rbf"", but this is currently not implemented."
6412,"def _timestamp(when):
    """"""
    Python 2 compatibility for `datetime.timestamp()`.
    """"""
    return (time.mktime(when.timetuple()) if sys.version_info < (3,) else
            when.timestamp())",python,Python 2 compatibility for `datetime.timestamp()`.,47,"This code defines a function `_timestamp(when)`, which provides compatibility for the `datetime.timestamp()` function in Python 2. The function takes a datetime object `when` as an argument. If the Python version is less than 3, it uses `time.mktime(when.timetuple())` to get the timestamp. If the Python version is 3 or greater, it uses the built-in `when.timestamp()` method to get the timestamp."
19769,"def show_approx(self, numfmt='%.3g'):
        """"""Show the probabilities rounded and sorted by key, for the
        sake of portable doctests.""""""
        return ', '.join([('%s: ' + numfmt) % (v, p)
                          for (v, p) in sorted(self.prob.items())])",python,"Show the probabilities rounded and sorted by key, for the
        sake of portable doctests.",68,"This Python function, `show_approx`, is a method of a class that displays the probabilities stored in the `prob` dictionary of the class. The probabilities are rounded and sorted by their keys. The rounding precision is determined by the `numfmt` parameter, which defaults to 3 decimal places. The function returns a string representation of the sorted dictionary, where each key-value pair is formatted as 'key: value' and pairs are separated by commas."
20959,"def timestamp_with_tzinfo(dt):
    """"""
    Serialize a date/time value into an ISO8601 text representation
    adjusted (if needed) to UTC timezone.

    For instance:
    >>> serialize_date(datetime(2012, 4, 10, 22, 38, 20, 604391))
    '2012-04-10T22:38:20.604391Z'
    """"""
    utc = tzutc()

    if dt.tzinfo:
        dt = dt.astimezone(utc).replace(tzinfo=None)
    return dt.isoformat() + 'Z'",python,"Serialize a date/time value into an ISO8601 text representation
    adjusted (if needed) to UTC timezone.

    For instance:
    >>> serialize_date(datetime(2012, 4, 10, 22, 38, 20, 604391))
    '2012-04-10T22:38:20.604391Z'",126,"This Python function, `timestamp_with_tzinfo`, takes a datetime object `dt` as an input and returns its ISO8601 text representation in UTC timezone. If the input datetime object has timezone information, it first converts it to UTC timezone and then removes the timezone information. Finally, it converts the datetime object to an ISO8601 formatted string and appends 'Z' to indicate it's in UTC timezone."
16533,"def read_bin_particle_density(self):
        """"""Read the bin particle density

        :returns: float
        """"""
        config = []

        # Send the command byte and sleep for 10 ms
        self.cnxn.xfer([0x33])
        sleep(10e-3)

        # Read the config variables by sending 256 empty bytes
        for i in range(4):
            resp = self.cnxn.xfer([0x00])[0]
            config.append(resp)

        bpd = self._calculate_float(config)

        return bpd",python,"Read the bin particle density

        :returns: float",114,"This Python function, `read_bin_particle_density`, is designed to read the binary particle density from a device. It first sends a command byte to the device and then pauses for 10 milliseconds. After that, it sends 256 empty bytes to the device in four iterations and stores the response in a list named `config`. The function then calculates the binary particle density using the `_calculate_float` method with `config` as an argument. The calculated binary particle density is returned as a float."
9226,"def public(function):
    """"""
    Decorator for public views that do not require authentication
    Sets an attribute in the fuction STRONGHOLD_IS_PUBLIC to True
    """"""
    orig_func = function
    while isinstance(orig_func, partial):
        orig_func = orig_func.func
    set_view_func_public(orig_func)

    return function",python,"Decorator for public views that do not require authentication
    Sets an attribute in the fuction STRONGHOLD_IS_PUBLIC to True",70,"This code defines a decorator named `public`. Decorators in Python are used to modify the behavior of a function or class. In this case, the `public` decorator is used for views that do not require authentication. It sets an attribute `STRONGHOLD_IS_PUBLIC` to `True` for the function it decorates. The `while` loop is used to handle cases where the function is a `partial` function, which is a function that has been partially applied to some arguments. The `set_view_func_public` function is then called with the original function as an argument. The original function is returned at the end."
16248,"def merge_overlapping_in_list(l):
    '''Sorts list, merges any overlapping intervals, and also adjacent intervals. e.g.
       [0,1], [1,2] would be merge to [0,.2].'''
    i = 0
    l.sort()

    while i < len(l) - 1:
        u = l[i].union(l[i+1])
        if u is not None:
            l[i] = u
            l.pop(i+1)
        else:
            i += 1",python,"Sorts list, merges any overlapping intervals, and also adjacent intervals. e.g.
       [0,1], [1,2] would be merge to [0,.2].",110,"This code defines a function called `merge_overlapping_in_list` that takes a list of intervals as input. It first sorts the list in ascending order. Then, it iterates through the sorted list, checking for overlapping or adjacent intervals. If it finds any, it merges them into a single interval. The merging is done using the `union` method, which returns a new interval that covers both the original intervals. If no overlap or adjacency is found between the current interval and the next one, it moves on to the next interval. The process continues until all intervals in the list have been checked."
16586,"async def release_control(self):
        """"""Release control of QTM.
        """"""

        cmd = ""releasecontrol""
        return await asyncio.wait_for(
            self._protocol.send_command(cmd), timeout=self._timeout
        )",python,Release control of QTM.,45,"This is an asynchronous function named `release_control` in Python. It's purpose is to release control of QTM (Qualisys Track Manager). The function creates a command string ""releasecontrol"", then it sends this command using the `_protocol` object's `send_command` method. The `asyncio.wait_for` function is used to set a timeout for this operation, which is specified by `self._timeout`. The function will return the result of the `send_command` operation."
14701,"def attach_volume(self, datacenter_id, server_id, volume_id):
        """"""
        Attaches a volume to a server.

        :param      datacenter_id: The unique ID of the data center.
        :type       datacenter_id: ``str``

        :param      server_id: The unique ID of the server.
        :type       server_id: ``str``

        :param      volume_id: The unique ID of the volume.
        :type       volume_id: ``str``

        """"""
        data = '{ ""id"": ""' + volume_id + '"" }'

        response = self._perform_request(
            url='/datacenters/%s/servers/%s/volumes' % (
                datacenter_id,
                server_id),
            method='POST',
            data=data)

        return response",python,"Attaches a volume to a server.

        :param      datacenter_id: The unique ID of the data center.
        :type       datacenter_id: ``str``

        :param      server_id: The unique ID of the server.
        :type       server_id: ``str``

        :param      volume_id: The unique ID of the volume.
        :type       volume_id: ``str``",166,"This code defines a function `attach_volume` that attaches a volume to a server in a data center. The function takes three parameters: `datacenter_id`, `server_id`, and `volume_id`, all of which are strings representing unique IDs. It then creates a JSON string `data` containing the `volume_id`. A POST request is made using the `_perform_request` method, with the URL constructed using the `datacenter_id` and `server_id`. The function returns the response from this request."
12553,"def new_account(self, label=None):
        """"""
        Creates new account, appends it to the :class:`Wallet`'s account list and returns it.

        :param label: account label as `str`
        :rtype: :class:`Account`
        """"""
        acc, addr = self._backend.new_account(label=label)
        assert acc.index == len(self.accounts)
        self.accounts.append(acc)
        return acc",python,"Creates new account, appends it to the :class:`Wallet`'s account list and returns it.

        :param label: account label as `str`
        :rtype: :class:`Account`",86,"This code defines a method called `new_account` in a class (likely named `Wallet`). This method creates a new account using the `_backend.new_account` method, passing an optional label. It then checks if the index of the new account is equal to the length of the existing accounts list, which should be true if the account is new. The new account is then appended to the list of accounts in the `Wallet` and returned."
19438,"def spawn(self, generations):
        """"""Grow this Pantheon by multiplying Gods.""""""
        egg_donors = [god for god in self.gods.values() if god.chromosomes == 'XX']
        sperm_donors = [god for god in self.gods.values() if god.chromosomes == 'XY']

        for i in range(generations):
            print(""\nGENERATION %d\n"" % (i+1))
            gen_xx = []
            gen_xy = []

            for egg_donor in egg_donors:
                sperm_donor = random.choice(sperm_donors)
                brood = self.breed(egg_donor, sperm_donor)

                for child in brood:
                    if child.divinity > human:
                        # divine offspring join the Pantheon
                        self.add_god(child)
                    if child.chromosomes == 'XX':
                        gen_xx.append(child)
                    else:
                        gen_xy.append(child)

            # elder gods leave the breeding pool
            egg_donors = [ed for ed in egg_donors if ed.generation > (i-2)]
            sperm_donors = [sd for sd in sperm_donors if sd.generation > (i-3)]

            # mature offspring join the breeding pool
            egg_donors += gen_xx
            sperm_donors += gen_xy",python,Grow this Pantheon by multiplying Gods.,272,"This code defines a method called `spawn` in a class, which simulates the growth of a pantheon (a group of gods) over a certain number of generations. It first identifies gods with 'XX' chromosomes as egg donors and gods with 'XY' chromosomes as sperm donors. For each generation, it pairs each egg donor with a random sperm donor and breeds them to produce offspring. If an offspring's divinity is greater than a human's, it is added to the pantheon. The offspring are also added to the appropriate donor list based on their chromosomes. After each generation, gods that are too old are removed from the donor lists, and the new generation of gods is added to the donor lists."
14179,"def click(self, node):
        
        """""" Callback from graph.events when a node is clicked.
        """"""
        
        if not self.has_node(node.id): return
        if node == self.root: return
        
        self._dx, self._dy = self.offset(node)
        self.previous = self.root.id
        self.load(node.id)",python,Callback from graph.events when a node is clicked.,69,"This code defines a method called `click` that is triggered when a node in a graph is clicked. The method first checks if the clicked node exists in the graph and if the clicked node is the root node. If either of these conditions is true, the method returns without doing anything. If both conditions are false, it calculates the offset of the clicked node from the current position and stores it. It also stores the id of the root node in a variable called `previous`. Finally, it loads the clicked node."
4251,"def build_tool(self, doc, entity):
        """"""Builds a tool object out of a string representation.
        Returns built tool. Raises SPDXValueError if failed to extract
        tool name or name is malformed
        """"""
        match = self.tool_re.match(entity)
        if match and validations.validate_tool_name(match.group(self.TOOL_NAME_GROUP)):
            name = match.group(self.TOOL_NAME_GROUP)
            return creationinfo.Tool(name)
        else:
            raise SPDXValueError('Failed to extract tool name')",python,"Builds a tool object out of a string representation.
        Returns built tool. Raises SPDXValueError if failed to extract
        tool name or name is malformed",104,"This code defines a method named `build_tool` that takes two parameters: `doc` and `entity`. The method is designed to construct a tool object from a string representation. It uses a regular expression to match the `entity` and validate the tool name. If the tool name is valid, it creates a new tool object with the extracted name and returns it. If the tool name is not valid or cannot be extracted, it raises an `SPDXValueError` with a message indicating the failure to extract the tool name."
8944,"def _enroll_users(
            cls,
            request,
            enterprise_customer,
            emails,
            mode,
            course_id=None,
            program_details=None,
            notify=True
    ):
        """"""
        Enroll the users with the given email addresses to the courses specified, either specifically or by program.

        Args:
            cls (type): The EnterpriseCustomerManageLearnersView class itself
            request: The HTTP request the enrollment is being created by
            enterprise_customer: The instance of EnterpriseCustomer whose attached users we're enrolling
            emails: An iterable of strings containing email addresses to enroll in a course
            mode: The enrollment mode the users will be enrolled in the course with
            course_id: The ID of the course in which we want to enroll
            program_details: Details about a program in which we want to enroll
            notify: Whether to notify (by email) the users that have been enrolled
        """"""
        pending_messages = []

        if course_id:
            succeeded, pending, failed = cls.enroll_users_in_course(
                enterprise_customer=enterprise_customer,
                course_id=course_id,
                course_mode=mode,
                emails=emails,
            )
            all_successes = succeeded + pending
            if notify:
                enterprise_customer.notify_enrolled_learners(
                    catalog_api_user=request.user,
                    course_id=course_id,
                    users=all_successes,
                )
            if succeeded:
                pending_messages.append(cls.get_success_enrollment_message(succeeded, course_id))
            if failed:
                pending_messages.append(cls.get_failed_enrollment_message(failed, course_id))
            if pending:
                pending_messages.append(cls.get_pending_enrollment_message(pending, course_id))

        if program_details:
            succeeded, pending, failed = cls.enroll_users_in_program(
                enterprise_customer=enterprise_customer,
                program_details=program_details,
                course_mode=mode,
                emails=emails,
            )
            all_successes = succeeded + pending
            if notify:
                cls.notify_program_learners(
                    enterprise_customer=enterprise_customer,
                    program_details=program_details,
                    users=all_successes
                )
            program_identifier = program_details.get('title', program_details.get('uuid', _('the program')))
            if succeeded:
                pending_messages.append(cls.get_success_enrollment_message(succeeded, program_identifier))
            if failed:
                pending_messages.append(cls.get_failed_enrollment_message(failed, program_identifier))
            if pending:
                pending_messages.append(cls.get_pending_enrollment_message(pending, program_identifier))

        cls.send_messages(request, pending_messages)",python,"Enroll the users with the given email addresses to the courses specified, either specifically or by program.

        Args:
            cls (type): The EnterpriseCustomerManageLearnersView class itself
            request: The HTTP request the enrollment is being created by
            enterprise_customer: The instance of EnterpriseCustomer whose attached users we're enrolling
            emails: An iterable of strings containing email addresses to enroll in a course
            mode: The enrollment mode the users will be enrolled in the course with
            course_id: The ID of the course in which we want to enroll
            program_details: Details about a program in which we want to enroll
            notify: Whether to notify (by email) the users that have been enrolled",526,"This Python function, `_enroll_users`, is designed to enroll users into a course or a program in an enterprise learning system. It takes in several parameters including the request, the enterprise customer, the emails of the users to be enrolled, the mode of enrollment, the course ID, program details, and a boolean to determine if the users should be notified of their enrollment.

If a course ID is provided, the function attempts to enroll the users in the specified course. It then checks if the enrollment was successful, failed, or pending, and appends the appropriate message to the `pending_messages` list. If the `notify` parameter is set to `True`, it sends a notification to the enrolled users.

If program details are provided, the function attempts to enroll the users in the specified program. Similar to the course enrollment, it checks the status of the enrollment and appends the appropriate message to the `pending_messages` list. If the `notify` parameter is set to `True`, it sends a notification to the enrolled users.

Finally, it sends all the messages in the `pending_messages` list."
